## Overview
A curated survey of OCR in the era of large language models, covering visual text parsing, understanding, benchmarks, challenges, and perspective. Paper is coming soon. 


## ðŸŽ‰ News
- **[2026-2-11]** ðŸ”¥ We release an open-source resource to help the community easily track recent OCR research!

## ðŸ“– Contents
- [News](#-news)
- [Visual Text Parsing](#-visual-text-parsing)
- [Visual Text Understanding](#-visual-text-understanding)
- [Benchmarks and Evaluation](#-benchmarks-and-evaluation)

## ðŸ“„ Visual Text Parsing
##### End-to-end
| Date | Name | ModelType | Title | Paper | GitHub |
|:-:|:-:|:-:|:-:|:-:|:-:|
| 2025-10 | `OLMOCR 2` | VLM Â· End-to-end | olmOCR 2: Unit Test Rewards for Document OCR | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.19817) | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) |
| 2025-10 | `Chandra v0.1.0` | VLM Â· End-to-end | â€” | â€” | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/chandra?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/chandra) |
| 2025-10 | `DeepSeek-OCR` | VLM Â· End-to-end | DeepSeek-OCR: Contexts Optical Compression | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.18234) | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-OCR) |
| 2025-10 | `Infinity-Parser` | VLM Â· End-to-end | Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.03197) | [![GitHub Stars](https://img.shields.io/github/stars/infly-ai/INF-MLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/infly-ai/INF-MLLM) |
| 2025-10 | `Nanonets-OCR 2` | VLM Â· End-to-end | Transforming documents into LLM-ready structured data | [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://nanonets.com/research/nanonets-ocr-2/) | [![GitHub Stars](https://img.shields.io/github/stars/NanoNets/docstrange?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/NanoNets/docstrange) |
| 2025-09 | `Logics-Parsing` | VLM Â· End-to-end | Logics-Parsing Technical Report | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.19760) | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Logics-Parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/Logics-Parsing) |
| 2025-09 | `Granite-Docling-258M` | VLM Â· End-to-end | â€” | [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://huggingface.co/ibm-granite/granite-docling-258M) | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) |
| 2025-07 | `dots.ocr` | VLM Â· End-to-end | dots.ocr: Multilingual Document Layout Parsing in a Single VLM | [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://github.com/rednote-hilab/dots.ocr/blob/master/assets/blog.md) | [![GitHub Stars](https://img.shields.io/github/stars/rednote-hilab/dots.ocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/rednote-hilab/dots.ocr) |
| 2025-06 | `OCRFlux` | VLM Â· End-to-end | OCRFlux: Mastering Complex Layouts and Seamless Page Merging | [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://ocrflux.pdfparser.io/#/blog) | [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) |
| 2025-06 | `MinerU2.0-2505-0.9B` | VLM Â· End-to-end | â€” | â€” | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) |
| 2025-03-14 | `SmolDocling-256M` | VLM Â· End-to-end | SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.11576) | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) |
| 2025-02 | `OLMOCR` | VLM Â· End-to-end | olmOCR: Unlocking Trillions of Tokens in PDFs with VLMs | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) |
| 2025-01 | `Ocean-OCR` | VLM Â· End-to-end | Ocean-OCR: Towards General OCR Application via a Vision-Language Model | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2501.15558) | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) |
| 2024-09 | `GOT-OCR 2.0` | VLM Â· End-to-end | General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.01704) | [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/GOT-OCR2.0?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/GOT-OCR2.0) |
| 2023-08 | `Nougat` | End-to-end | Nougat: Neural Optical Understanding for Academic Documents | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.13418) | [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/nougat?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/facebookresearch/nougat) |
##### Pipeline
| Date | Name | ModelType | Title | Paper | GitHub |
|:-:|:-:|:-:|:-:|:-:|:-:|
| 2025-10 | `PaddleOCR-VL` | VLM Â· Pipline | PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact VLM | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.14528) | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) |
| 2025-10 | `Dolphin-1.5` | VLM Â· Pipline | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2505.14059) | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) |
| 2025-09 | `Marker 1.10.1` | Pipeline | â€” | â€” | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) |
| 2025-09 | `MinerU 2.5` | VLM Â· Pipline | MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Res Document Parsing | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.22186) | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) |
| 2025-07 | `MonkeyOCR-Pro` | VLM Â· Pipline | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) |
| 2025-06 | `MonkeyOCR` | VLM Â· Pipline | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) |
| 2025-05 | `Dolphin` | VLM Â· Pipline | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | <a href="https://aclanthology.org/2025.findings-acl.1130//"><img src="./figs/ACL-logo.png" width="80"></a> | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) |
| 2025-02 | `PP-StructureV3` | Pipeline | PaddleOCR 3.0 Technical Report | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/html/2507.05595v1) | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) |
| 2024-12 | `MarkItDown` | Pipeline | â€” | â€” | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/markitdown?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/markitdown) |
| 2024-09 | `MinerU` | Pipeline | MinerU: An Open-Source Solution for Precise Document Content Extraction | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.18839) | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) |
| 2024-03 | `open-parse` | Pipeline | â€” | â€” | [![GitHub Stars](https://img.shields.io/github/stars/Filimoa/open-parse?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Filimoa/open-parse) |
| 2022-10 | `unstructured` | Pipeline | Transform complex, unstructured data into clean, structured data. Securely. Continuously. Effortlessly. | [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://unstructured.io/) | [![GitHub Stars](https://img.shields.io/github/stars/Unstructured-IO/unstructured?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Unstructured-IO/unstructured) |
| 2022-07 | `RapidOCR` | Pipeline | â€” | â€” | [![GitHub Stars](https://img.shields.io/github/stars/RapidAI/RapidOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/RapidAI/RapidOCR) |

## ðŸ“„ Visual Text Understanding
| Date | Name | ModelType | Title | Paper | GitHub |
|:-:|:-:|:-|:-:|:-:|:-:|
| 2024-09 | `mPLUG-DocOwl2` | VLM Â· End-to-end | mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding | <a href="https://aclanthology.org/2025.acl-long.291/"><img src="./figs/ACL-logo.png" width="80"></a> | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2) |
| 2024-05 | `Fox` | VLM Â· End-to-end | Focus Anywhere for Fine-grained Multi-page Document Understanding | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.14295) | [![GitHub Stars](https://img.shields.io/github/stars/ucaslcl/Fox?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ucaslcl/Fox) |
| 2024-03 | `mPLUG-DocOwl 1.5` | VLM Â· End-to-end | mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding | <a href="https://aclanthology.org/2024.findings-emnlp.175/"><img src="./figs/EMNLP-logo.png" width="80"></a> | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5) |
| 2023-12 | `Vary` | VLM Â· End-to-end | Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models | <a href="https://eccv.ecva.net/virtual/2024/poster/2237"><img src="./figs/ECCV-logo.svg" width="80"></a> | [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/Vary?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/Vary) |
| 2023-9 | `KOSMOS-2.5` | VLM Â· End-to-end | KOSMOS-2.5: A Multimodal Literate Model | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/unilm?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/unilm/tree/master/kosmos-2.5) |
| 2023-07 | `mPLUG-DocOwl` | VLM Â· End-to-end | mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding | [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2307.02499) | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl) |

## ðŸ“„ Benchmarks and Evaluation
### 
| **Benchmark Name**   | **Description**                                              | **Link**                                                     |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Omnidocbench (v1.5)  | Contains 1355 PDF pages and over 100,000 fine-grained annotations (70k+ span-level, 20k+ block-level). <br />Compared to v1.0, it balances the ratio of Chinese/English pages and increases the resolution of some document types. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/opendatalab/OmniDocBench/tree/main) [![GitHub](https://img.shields.io/github/stars/opendatalab/OmniDocBench?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/OmniDocBench) |
| OCRFlux-bench-single | Contains 2,000 PDF page samples. Each sample includes one PDF page and its corresponding ground-truth Markdown text. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-single) |
| OCRFlux-bench-cross  | Contains 1,000 samples. Each sample includes a list of Markdown elements from two consecutive pages. The main purpose is to evaluate the model's ability to accurately detect and merge these cross-page elements. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-cross) |
| Omnidocbench (v1.0)  | Contains 981 PDF pages and 100,000+ fine-grained annotations (70k+ span-level, 20k+ block-level). <br />Supported tasks: End-to-end (E2E) parsing, layout detection, table recognition, formula recognition, and text OCR. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/opendatalab/OmniDocBench/tree/v1_0) [![arXiv](https://img.shields.io/badge/arXiv-2412.07626-b31b1b.svg)](https://arxiv.org/abs/2412.07626) |
| olmOCR-Bench         | Contains 1,402 PDF documents. Uses 7,000 deterministic unit tests (evaluating pass/fail) as the evaluation standard, distinct from quantitative metrics.Covers 7 document types and various internal categories. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/allenai/olmOCR-bench) [![arXiv](https://img.shields.io/badge/arXiv-2502.18443-b31b1b.svg)](https://arxiv.org/abs/2502.18443) [![GitHub](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) |
| Fox Benchmark        | Contains 944 interfaces (329M). Designed to test the *focusing ability* of Large Vision Language Models (VLMs) on dense PDF documents. Includes 9 different sub-tasks. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ucaslcl/Fox_benchmark_data/tree/main) [![GitHub](https://img.shields.io/github/stars/ucaslcl/Fox?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ucaslcl/Fox) |
| Ocean-OCR bench      | Includes 1160 images, categorized into bilingual (Chinese and English) and three text types (document, handwritten, and scene) | [![GitHub](https://img.shields.io/github/stars/guoxy25/Ocean-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) |
| READoc               | Contains 3576 documents across three subsets: READOC-arXiv (1009), READOC-GitHub (1224), READOC-Zenodo (1343). <br />Introduces an "Evaluation S3uite" (Standardization, Segmentation, Scoring) for automated E2E DSE task evaluation. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/lazyc/READoc) [![arXiv](https://img.shields.io/badge/arXiv-2409.05137-b31b1b.svg)](https://arxiv.org/abs/2409.05137) |
| marker_benchmark     | Contains 2138 documents (761M). Includes 11 different document types. <br />Provides two types of annotation formats: heuristic and LLM. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/datalab-to/marker_benchmark) [![GitHub](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) |
| OCRBench_v2          | Contains 10,000 human-verified Q&A pairs with a high proportion of difficult samples. A benchmark with comprehensive task coverage and broad scenario coverage (31 scenarios, e.g., street view, receipts, formulas, charts). | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ling99/OCRBench_v2) [![GitHub](https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MultimodalOCR/blob/main/OCRBench_v2/README.md) |
| CC-OCR               | Contains 39 subsets with 7058 fully annotated images, covering four core OCR task tracks.<br />Extend evaluation to three tracks: OCR performance, document parsing, and KIE | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/wulipc/CC-OCR) [![GitHub](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Benchmarks/CC-OCR)[![arXiv](https://img.shields.io/badge/arXiv-2412.02210-b31b1b.svg)](https://arxiv.org/abs/2412.02210) |
| dots.ocr-bench       | (Not open-sourced)                                           |                                                              |
| Dolphin-Page         | (Not open-sourced) A bilingual benchmark for complex document parsing. Contains 210 document pages, including 111 text-only documents and 99 complex samples (tables, math, charts) with single and multi-column layouts. | [![GitHub](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) [![arXiv](https://img.shields.io/badge/arXiv-2505.14059-b31b1b.svg)](https://arxiv.org/abs/2505.14059) |
| LogicsParsingBench   | (Not open-sourced)                                           |                                                              |
| OCRBench (v1)        | Contains 1000 Q&A pairs. <br />Consists of five components: text recognition, Scene Text-Centric VQA, Document-Oriented VQA, KIE, and HMER. | [![GitHub](https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MultimodalOCR?tab=readme-ov-file) [![arXiv](https://img.shields.io/badge/arXiv-2305.07895-b31b1b.svg)](https://arxiv.org/abs/2305.07895) |

### Dataset

| **Dataset Name**       | **Description**                                              | **Link**                                                     |
| ---------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| olmOCR-synthmix-1025   | A dataset of 2,186 single PDF pages, used to create 30,381 synthetic benchmark cases formatted according to olmOCR-bench. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/allenai/olmOCR-synthmix-1025) |
| Infinity-Doc-55K       | Comprises 55K real-world and synthetic scanned documents. <br />Data composition: Synthetic Documents (6.5k), Financial Reports (16.1k), Medical Reports (5k), Academic Papers (8.9k), Books (10.5k), Magazines (3k), Web Pages (5k). | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/infly/Infinity-Doc-55K) |
| olmOCR-mix-0225        | Contains 105,504 unique documents, totaling 266,135 pages from two sources: <br />1. Web crawled PDFs (99,903 docs, 249,332 pages)<br /> 2. Internet Archive books (5,601 docs, 16,803 pages) | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/allenai/olmOCR-mix-0225) |
| SROIE-document-parsing | Contains 1000 complete scanned receipt images. Each receipt image includes approximately four key text fields for KIE. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/hf-tuner/SROIE-document-parsing) [![GitHub](https://img.shields.io/github/stars/zzzDavid/ICDAR-2019-SROIE?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zzzDavid/ICDAR-2019-SROIE) |
| finepdfs               | The largest public corpus sourced *from* PDFs, containing ~3 trillion tokens from 475M documents in 1733 languages. This is a text/table corpus (for Text Generation tasks), not raw PDF images for OCR. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/HuggingFaceFW/finepdfs) |

