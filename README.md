## Overview
A curated survey of OCR in the era of large language models, covering visual text parsing, understanding, benchmarks, challenges, and perspective. 


## üéâ News
- **[2026-2-11]** üî• We release an open-source resource to help the community easily track recent OCR research!

## üìñ Contents
- [Overview](#overview)
- [üéâ News](#-news)
- [üìñ Contents](#-contents)
- [üìÑ Visual Text Parsing](#-visual-text-parsing)
- [üìÑ Visual Text Understanding](#-visual-text-understanding)
- [üìÑ Benchmarks and Evaluation](#-benchmarks-and-evaluation)
- [üìÑ Specialized Model](#-specialized-model)
  - [üìÑ Document Dewarping](#-document-dewarping)
  - [üìÑ Physical Structure Analysis](#-physical-structure-analysis)
  - [üìÑ Reading Order Prediction](#-reading-order-prediction)
  - [üìÑ Mathematical Expression Recognition](#-mathematical-expression-recognition)
  - [üìÑ Table Understanding](#-table-understanding)
  - [üìÑ Table Recognition](#-table-recognition)
  - [üìÑ Chart Understanding](#-chart-understanding)
  - [üìÑ Scene Text Spotting](#-scene-text-spotting)
- [Contributing](#contributing)

## üìÑ Visual Text Parsing
| Venue | Name | Primary affiliation | Title | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| - | `GLM-OCR` | Z.ai | - | [![GitHub Stars](https://img.shields.io/github/stars/zai-org/GLM-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zai-org/GLM-OCR) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2602.05384) | `Dolphin-2.0` | ByteDance | Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.20552) | `DeepSeek-OCR 2` | DeepSeek-AI | DeepSeek-OCR 2: Visual Causal Flow | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR-2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-OCR-2) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.21639) | `OCRVerse` | Meituan | OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models | [![GitHub Stars](https://img.shields.io/github/stars/DocTron-hub/OCRVerse?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/DocTron-hub/OCRVerse) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.20430) | `Youtu-Parsing` | Tencent | Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding | [![GitHub Stars](https://img.shields.io/github/stars/TencentCloudADP/youtu-parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/TencentCloudADP/youtu-parsing) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.21957) | `PaddleOCR-VL 1.5` | Baidu Inc. | PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.15098) | `Uni-Parser` | DP Technology | Uni-Parser Technical Report | [![GitHub](https://img.shields.io/github/stars/uni-parser/uni-parser.github.io?style=for-the-badge&logo=github&label=GitHub&color=black)](https://uni-parser.github.io/) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.01248) | `TRivia` | The University of HongKong | TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognitio | [![GitHub](https://img.shields.io/github/stars/opendatalab/TRivia?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/TRivia) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.10619) | `DOCR-Inspector` | Peking University | DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM| [![GitHub](https://img.shields.io/github/stars/ZZZZZQT/DOCR-Inspector?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ZZZZZQT/DOCR-Inspector) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.21095) | `UniRec-0.1B` | Fudan University | UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters| [![GitHub](https://img.shields.io/github/stars/Topdu/OpenOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Topdu/OpenOCR) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2511.19575) | `HunyuanOCR` | Tencent | HunyuanOCR Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/Tencent-Hunyuan/HunyuanOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Tencent-Hunyuan/HunyuanOCR) | Nov. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.10390) | `MonkeyOCR v1.5` | KingSoft Office | MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Nov. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2510.21603) | `Doc-Researcher` | Huawei | Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research |  | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.19817) | `OLMOCR 2` | Allen Institute for AI | olmOCR 2: Unit Test Rewards for Document OCR | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Oct. 2025 |
| ‚Äî | `Chandra v0.1.0` | Datalab | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/chandra?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/chandra) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.18234) | `DeepSeek-OCR` | DeepSeek-AI | DeepSeek-OCR: Contexts Optical Compression | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-OCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.03197) | `Infinity-Parser` | INFLY Tech | Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/infly-ai/INF-MLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/infly-ai/INF-MLLM) | Oct. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://nanonets.com/research/nanonets-ocr-2/) | `Nanonets-OCR 2` | Nanonets | Transforming documents into LLM-ready structured data | [![GitHub Stars](https://img.shields.io/github/stars/NanoNets/docstrange?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/NanoNets/docstrange) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.14528) | `PaddleOCR-VL` | Baidu Inc. | PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact VLM | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2505.14059) | `Dolphin-1.5` | ByteDance | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.19760) | `Logics-Parsing` | Alibaba Group | Logics-Parsing Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Logics-Parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/Logics-Parsing) | Sep. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://huggingface.co/ibm-granite/granite-docling-258M) | `Granite-Docling-258M` | IBM | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) | Sep. 2025 |
| ‚Äî | `Marker 1.10.1` | Datalab | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.22186) | `MinerU 2.5` | Shanghai Artificial Intelligence Laboratory | MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Res Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2508.13238?) | `DianJin-OCR-R1` | Qwen DianJin Team | DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model | [![GitHub Stars](https://img.shields.io/github/stars/aliyun/qwen-dianjin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/aliyun/qwen-dianjin) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.02498)| `dots.ocr` | Xiaohongshu Inc. | dots.ocr: Multilingual Document Layout Parsing in a Single VLM | [![GitHub Stars](https://img.shields.io/github/stars/rednote-hilab/dots.ocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/rednote-hilab/dots.ocr) | Jul. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | `MonkeyOCR-Pro` | HUST | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Jul. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://ocrflux.pdfparser.io/#/blog) | `OCRFlux` | ChatDOC | OCRFlux: Mastering Complex Layouts and Seamless Page Merging | [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| ‚Äî | `MinerU2.0-2505-0.9B` | Shanghai Artificial Intelligence Laboratory | | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | `MonkeyOCR` | HUST | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Jun. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1130//"><img src="./figs/ACL-logo.png" width="80"></a> | `Dolphin` | ByteDance | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | May. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2506.07553)| `GTR-VL` | Shanghai Artificial Intelligence Laborator | GTR-VL: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition | - | May. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.11576) | `SmolDocling-256M` | IBM | SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) | Mar. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `OLMOCR` | Allen Institute for AI | olmOCR: Unlocking Trillions of Tokens in PDFs with VLMs | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/html/2507.05595v1) | `PP-StructureV3` | Baidu Inc. | PaddleOCR 3.0 Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2501.15558) | `Ocean-OCR` | Baichuan Inc. | Ocean-OCR: Towards General OCR Application via a Vision-Language Model | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) | Jan. 2025 |
| ‚Äî | `MarkItDown` | Microsoft | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/markitdown?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/markitdown) | Dec. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.01704) | `GOT-OCR 2.0` | StepFun | General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model | [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/GOT-OCR2.0?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/GOT-OCR2.0) | Sep. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.18839) | `MinerU` | Shanghai Artificial Intelligence Laboratory | MinerU: An Open-Source Solution for Precise Document Content Extraction | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Sep. 2024 |
| ‚Äî | `open-parse` | - | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/Filimoa/open-parse?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Filimoa/open-parse) | Mar. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.13418) | `Nougat` | Meta AI | Nougat: Neural Optical Understanding for Academic Documents | [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/nougat?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/facebookresearch/nougat) | Aug. 2023 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://unstructured.io/) | `unstructured` | Unstructured | Transform complex, unstructured data into clean, structured data. Securely. Continuously. Effortlessly. | [![GitHub Stars](https://img.shields.io/github/stars/Unstructured-IO/unstructured?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Unstructured-IO/unstructured) | Oct. 2022 |
| ‚Äî | `RapidOCR` | Unstructured | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/RapidAI/RapidOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/RapidAI/RapidOCR) | Jul. 2022 |


## üìÑ Visual Text Understanding
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.21631)  | `Qwen3-VL`| Alibaba Group |Qwen3-VL Technical Report |[![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/QwenLM/Qwen2.5-VL) |Nov. 2025|
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2508.18265) | `InternVL3.5`|  Shanghai AI Laboratory | InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/InternVL) |Apr. 2025|
| <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DocThinker_Explainable_Multimodal_Large_Language_Models_with_Rule-based_Reinforcement_Learning_ICCV_2025_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"> |`Doc Thinker` |HUST| DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding | --- |  Aug. 2025 |
| <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Janus_Decoupling_Visual_Encoding_for_Unified_Multimodal_Understanding_and_Generation_CVPR_2025_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `Janus` | DeepSeek-AI | Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/Janus) | Jun. 2025 |
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.10479) | `InternVL3`|  Shanghai AI Laboratory | InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/InternVL) |Apr. 2025|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.07491) | `Kimi-VL` | MoonshotAI | Kimi-VL: Vision-Language Model for Advanced Multimodal Understanding | [![GitHub Stars](https://img.shields.io/github/stars/MoonshotAI/Kimi-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/MoonshotAI/Kimi-VL) | Apr. 2025 |
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.04065) | `PP-DocBee`|  Baidu | PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleMIX?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleMIX) |Mar. 2025|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.01743) | `Phi-4-Mini` | Microsoft | Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/PhiCookBook?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/PhiCookBook) | Mar. 2025 |
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.16161) | `OmniParser V2`| HUST | OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models |---|Feb. 2025|
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2412.05271) | `InternVL2.5`|  Shanghai AI Laboratory | PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/InternVL) |Dec. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2412.10302) | `DeepSeek-VL2` | DeepSeek AI | DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-VL2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-VL2) | Dec. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2410.07073) | `Pixtral-12B` | Mistral AI | Pixtral 12B: A Vision-Language Model for Advanced Multimodal Understanding | [![GitHub Stars](https://img.shields.io/github/stars/mistralai/mistral-inference?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mistralai/mistral-inference/) | Oct. 2024 |
| <a href="https://aclanthology.org/2025.acl-long.291/"><img src="./figs/ACL-logo.png" width="80"></a> | `mPLUG-DocOwl2` | Alibaba Group | mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2) | Sept. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.17146) | `Molmo & PixMo` | Allen Institute for AI | Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models | [![GitHub Stars](https://img.shields.io/github/stars/allenai/molmo?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/molmo) | Sep. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liao_DocLayLLM_An_Efficient_Multi-modal_Extension_of_Large_Language_Models_for_CVPR_2025_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"> |`DocLayLLM`| South China Univ. of Techn. + Alibaba Cloud|DocLayLLM: An Efficient Multi-modal Extension of Large Language Models for Text-rich Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/whlscut/DocLayLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whlscut/DocLayLLM) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.12637) | `Cog-iDEFICS3` | Hugging Face | Cog-iDEFICS3: Scaling Multimodal Models with Integrated Field-aware Embeddings and Semantic Compositionality | [![GitHub Stars](https://img.shields.io/github/stars/zsxkib/cog-idefics3?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zsxkib/cog-idefics3) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.15998) | `Eagle-X5` | NVIDIA | Eagle-X5: Scalable Vision-Language Models with Enhanced Cross-modal Alignment and Generative Capability | [![GitHub Stars](https://img.shields.io/github/stars/NVlabs/Eagle?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/NVlabs/Eagle) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2406.12793) | `GLM-4` | Zhipu AI | ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools | [![GitHub Stars](https://img.shields.io/github/stars/zai-org/GLM-4?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zai-org/GLM-4) | Jun. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2407.07895) | `LLaVA-NeXT` | ByteDance | LLaVA-NeXT: Next-Generation Large Vision-Language Models with Enhanced Multimodal Capabilities | [![GitHub Stars](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/LLaVA-VL/LLaVA-NeXT) | Jul. 2024 |
| <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/9ee3a664ccfeabc0da16ac6f1f1cfe59-Paper-Conference.pdf"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `Cambrian-1` | New York University | Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal Large Language Models | [![GitHub Stars](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge&logo=github)](https://github.com/cambrian-mllm/cambrian) | Jun. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2024/html/EMU2-chat_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `EMU2-chat` | Beijing Academy of Artificial Intelligence | EMU2-chat: Generative Multimodal Models are In-Context Learners | [![GitHub Stars](https://img.shields.io/github/stars/baaivision/Emu?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/baaivision/Emu) | Jun. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2403.04473) | `TextMonkey` | HUST | TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/Monkey?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/Monkey) | May. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.20797) | `Ovis` | Alibaba Group | Ovis: Structural Embedding Alignment for Multimodal Large Language Model | [![GitHub Stars](https://img.shields.io/github/stars/AIDC-AI/Ovis?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AIDC-AI/Ovis) | May 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.14295)  | `Fox` | University of Chinese Academy of Sciences | Focus Anywhere for Fine-grained Multi-page Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/ucaslcl/Fox?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ucaslcl/Fox) |May. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/html/2403.04652v1) | `Yi-VL` | 01.AI | Yi: Open Foundation Models by 01.AI ‚Äî Vision-Language Model | [![GitHub Stars](https://img.shields.io/github/stars/01-ai/Yi?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/01-ai/Yi) | Mar. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2404.09204)  | `TextHawk` | Zhejiang University|TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models | [![GitHub Stars](https://img.shields.io/github/stars/yuyq96/TextHawk?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/yuyq96/TextHawk) |Apr. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2404.06512) | `InternLM-XComposer2-4KHD` | Shanghai AI Laboratory | InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD | [![GitHub Stars](https://img.shields.io/github/stars/InternLM/InternLM-XComposer?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/InternLM/InternLM-XComposer) | Apr. 2024 |
|  <a href="https://aclanthology.org/2024.acl-long.463/"><img src="./figs/ACL-logo.png" width="80"></a> | `DocLLM` | JPMorgan AI Research | DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding|[![GitHub Stars](https://img.shields.io/github/stars/dswang2011/DocLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dswang2011/DocLLM) |  Jan. 2024 |
| <a href="https://aclanthology.org/2024.findings-emnlp.175/"><img src="./figs/EMNLP-logo.png" width="80"> |`mPLUG-DocOwl 1.5`| Alibaba Group | mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5) |Mar. 2024|
| <a href="https://eccv.ecva.net/virtual/2024/poster/2237"><img src="./figs/ECCV-logo.svg" width="80"></a> | `Vary`| MEGVII Technology |  Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models |  [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/Vary?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/Vary) |Dec. 2023|
| <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Monkey_Image_Resolution_and_Text_Label_Are_Important_Things_for_CVPR_2024_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"> |`Monkey`| HUST |Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models | [![GitHub Stars](https://img.shields.io/github/stars/whlscut/DocLayLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whlscut/DocLayLLM) | Nov. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2306.17107) | `LLaVAR` | Georgia Tech | LLaVAR: Enhanced Visual Instruction Tuning for Text-rich Image Understanding | [![GitHub Stars](https://img.shields.io/github/stars/SALT-NLP/LLaVAR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/SALT-NLP/LLaVAR) | Jun. 2023 |
| <a href="https://neurips.cc/virtual/2024/poster/96510"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `CogVLM` | ZaiOrg | CogVLM: Visual Expert for Pretrained Language Models | [![GitHub Stars](https://img.shields.io/github/stars/zai-org/CogVLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zai-org/CogVLM) | Nov. 2023 |
| <a href="https://aclanthology.org/2023.findings-emnlp.187/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `UReader` | East China Normal University | UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model | [![GitHub Stars](https://img.shields.io/github/stars/LukeForeverYoung/UReader?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/LukeForeverYoung/UReader) | Nov. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `KOSMOS-2.5`|Microsoft | KOSMOS-2.5: A Multimodal Literate Model| [![GitHub Stars](https://img.shields.io/github/stars/microsoft/unilm?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/unilm/tree/master/kosmos-2.5) | Sept. 2023|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.12966) | `QwenVL` | Alibaba Group | QwenVL: A Vision-Language Model for Unified Visual and Language Understanding | [![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/QwenLM/Qwen-VL) | Aug. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2307.02499) |  `mPLUG-DocOwl` | DAMO Academy, Alibaba Group |mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding |  [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl) |Jul. 2023|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2306.01733) | `DocFormerv2` | AWS AI Labs |DocFormerv2: Local Features for Document Understanding |---| Jun. 2023|

## üìÑ Benchmarks and Evaluation

| Venue | Benchmark Name | Description  | Link | Date |
|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2512.02498) | `dots.ocr-bench` | A multilingual end-to-end document parsing benchmark introduced to measure cross-lingual generalization beyond EN/ZH-centric benchmarks, constructed from real-world documents spanning 126 languages; the paper states the benchmark will be released and reports evaluation using edit-distance and table TEDS style metrics. | ‚Äî | Dec. 2025 |
| <a href="https://aclanthology.org/anthology-files/pdf/findings/2025.findings-emnlp.1318.pdf"><img src="./figs/EMNLP-logo.png" width="80"></a> | `TIU-Bench` | A benchmark for evaluating large multimodal models on text-rich image understanding. It covers diverse tasks that require reading and interpreting dense text content embedded in images, pushing models‚Äô abilities beyond simple OCR to include comprehension, reasoning, and contextual extraction from visually complex scenes. | - | Nov. 2025 |
| <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CC-OCR_A_Comprehensive_and_Challenging_OCR_Benchmark_for_Evaluating_Large_ICCV_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `CC-OCR` | A comprehensive OCR-centric benchmark for evaluating large multimodal models across four tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. It contains 39 subsets with 7,058 fully annotated images, 41% sourced from real-world applications, designed to assess fine-grained literacy, text grounding, multi-orientation robustness, and hallucination behavior. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/wulipc/CC-OCR) [![GitHub Stars](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Benchmarks/CC-OCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.19760) | `LogicsParsingBench` | A page-level document parsing benchmark introduced for evaluating complex layout analysis and reading order inference, consisting of 1,078 PDF page images spanning 9 major document categories and more than 20 sub-categories, including challenging layouts such as multi-column newspapers, posters, chemical formulas, and handwritten Chinese text. | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Logics-Parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/Logics-Parsing) | Sept. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1128/"><img src="./figs/ACL-logo.png" width="80"></a> | `READoc` | A unified benchmark for realistic document structured extraction that defines DSE as converting unstructured PDFs into semantically rich Markdown. The dataset contains 3,576 real-world documents sourced from arXiv, GitHub, and Zenodo, and is paired with an evaluation suite for standardization, segmentation, and scoring to support unified comparison across DSE systems. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/lazyc/READoc) [![GitHub Stars](https://img.shields.io/github/stars/icip-cas/READoc?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/icip-cas/READoc) | Jul. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1130/"><img src="./figs/ACL-logo.png" width="80"></a> | `Dolphin-Page` | A bilingual benchmark of 210 document pages designed for complex document image parsing and natural reading order reconstruction; it includes 111 pure-text documents and additional pages with mixed/complex layouts (e.g., tables, formulas, figures) to stress layout analysis and structured extraction. | [![GitHub Stars](https://img.shields.io/github/stars/ByteDance/Dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ByteDance/Dolphin) | Jul. 2025 |
| <a href="https://aclanthology.org/2025.acl-long.XXX/"><img src="./figs/ACL-logo.png" width="80"></a> | `LongDocURL` | A comprehensive multimodal long document benchmark integrating understanding, reasoning, and locating abilities for large vision-language models. It evaluates models on complex long-form documents with multimodal content, requiring not only answer generation but also precise evidence localization across extended contexts. | [![GitHub Stars](https://img.shields.io/github/stars/dengc2023/LongDocURL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dengc2023/LongDocURL) | Jul. 2025 |
| <a href="https://cvpr.thecvf.com/virtual/2025/poster/34400"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `OmniDocBench (v1.5)` | A document understanding benchmark with 1,355 PDF pages and over 100,000 fine-grained annotations (70k+ span-level, 20k+ block-level), balancing Chinese/English page ratio and increasing resolution for diverse document types, designed to evaluate OCR and structured extraction robustness. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/opendatalab/OmniDocBench/tree/main) [![GitHub](https://img.shields.io/github/stars/opendatalab/OmniDocBench?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/OmniDocBench) | Jun. 2025 |
| <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Docopilot_Improving_Multimodal_Models_for_Document-Level_Understanding_CVPR_2025_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `Doc750K` | A large-scale document-level multimodal understanding dataset designed for complex multi-page document comprehension, featuring diverse document structures, extensive cross-page dependencies, and real question-answer pairs grounded in original documents. It supports evaluation of coherence, accuracy, and multi-turn reasoning in document-level multimodal models without relying on retrieval-augmented pipelines. | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/Docopilot?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/Docopilot) | Jun. 2025 |
| ‚Äî | `OCRFlux-bench-single` | A single-page PDF-to-Markdown benchmark containing 2000 bilingual (EN/ZH) PDF pages with manually verified ground-truth Markdown, designed to evaluate complex layout parsing including multi-column text, tables, figures, and formulas. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-single) [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| ‚Äî | `OCRFlux-bench-cross` | A cross-page PDF-to-Markdown benchmark with 1000 bilingual (EN/ZH) samples, where each sample consists of Markdown element sequences from two consecutive pages and annotated index pairs indicating elements that should be merged, targeting evaluation of cross-page table and paragraph merging. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-cross) [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.03197) | `Infinity-Doc-55K` | A large-scale scanned document parsing benchmark combining 55K high-fidelity synthetic pages with expert-filtered real-world documents, designed to evaluate layout-aware parsing including OCR accuracy, table and formula extraction, paragraph segmentation, and reading order preservation across English and Chinese documents. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/infly/Infinity-Doc-55K) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `olmOCR-bench` | A unit-test style OCR evaluation benchmark with 1,403 PDF documents and 7,010 machine-verifiable pass/fail test cases, designed to stress challenging document phenomena such as formulas, tables, tiny fonts, old scans, and other layout and structure preservation issues in PDF-to-text extraction. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/allenai/olmOCR-bench) [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `olmOCR-mix-0225` | A large-scale PDF-to-text training dataset consisting of 260,000 pages sampled from over 100,000 crawled PDFs with diverse layouts and quality levels, including graphics, handwritten text, and low-quality scans, used to fine-tune a 7B vision-language model for structured, natural reading-order PDF extraction. | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2501.15558) | `Ocean-OCR-OCR_eval_data` | An evaluation suite for OCR practical scenarios covering (1) bilingual dense document extraction with 100 English paper images and 100 Chinese paper images, (2) scene text recognition with images sampled from MSRA-TD500 and manually verified ground truth (bootstrapped from PaddleOCR), and (3) multi-granularity bilingual handwritten recognition (real and synthetic; paragraph- and line-level), with 100 samples per category. | [![GitHub Stars](https://img.shields.io/github/stars/guoxy25/Ocean-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/guoxy25/Ocean-OCR) | Jan. 2025 |
| ‚Äî | `marker_benchmark` | An overall PDF-to-Markdown evaluation set built from single PDF pages extracted from Common Crawl, paired with block-level ground truth segments for scoring. The released dataset contains 2,138 page samples with fields including rendered page image and ground-truth blocks, and is used in Marker‚Äôs ‚ÄúOverall PDF Conversion‚Äù benchmark with heuristic alignment and LLM-judge scoring. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/datalab-to/marker_benchmark) [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) | Jan. 2025 |
| <a href="https://openreview.net/forum?id=Vb6i3Dp24N"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `OCRBench_v2` | A large-scale bilingual text-centric benchmark for evaluating LMM OCR capabilities beyond recognition, covering visual text localization and reasoning across 23 tasks and 31 diverse scenarios, with 10,000 human-verified question-answer pairs and a high proportion of difficult samples. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ling99/OCRBench_v2) [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MultimodalOCR) | Dec. 2024 |
| <a href="https://aclanthology.org/2024.findings-emnlp.175/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `DocLocal4K` | A text localization and recognition evaluation benchmark for OCR-free document understanding, containing 4,250 document image samples balanced across four granularities (word, phrase, line, block). It evaluates both text grounding (IOU@0.5) and text recognition (BLEU1‚ÄìBLEU4) to assess spatial preservation and fine-grained localization capability in document images. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/mPLUG/DocLocal4K) [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5) | Nov. 2024 |
| <a href="https://aclanthology.org/2024.emnlp-main.XXX/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `M-LongDoc` | A multimodal super-long document understanding benchmark with 851 samples drawn from recent documents spanning hundreds of pages, requiring open-ended explanations rather than purely extractive answers. It evaluates large multimodal models under retrieval-aware settings for long-context multimodal document reasoning. | ‚Äî | Nov. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2410.21311) | `MMDocBench` | An OCR-free document understanding benchmark for evaluating fine-grained visual perception and reasoning in large vision-language models. It defines 15 tasks with 4,338 QA pairs and 11,353 supporting regions across diverse document types including research papers, receipts, financial reports, tables, charts, and infographics. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/next-tat/MMDocBench) [![GitHub Stars](https://img.shields.io/github/stars/fengbinzhu/MMDocBench?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/fengbinzhu/MMDocBench) | Oct. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.14295) | `Fox-benchmark` | A bilingual fine-grained multi-page document understanding benchmark with 9 sub-tasks (page OCR, line-level OCR, color-guided OCR, region-level OCR/translation/summary, multi-page multi-region OCR, cross-page VQA). It contains 112 English pages and 100 Chinese pages (single/multi-column; >1,000 words per page), plus an additional 200 rendered interleaved pages for in-document figure caption evaluation. | [![GitHub Stars](https://img.shields.io/github/stars/ucaslcl/Fox?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ucaslcl/Fox) | May 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `OCREval` | A document-level OCR benchmark introduced with KOSMOS-2.5, consisting of 2,297 images collected from the test sets of 13 datasets, covering mathematical content, handwritten text, design images, receipts, digitally born documents, and web pages. It evaluates document-level text recognition using F1, IOU, and normalized edit distance metrics. | ‚Äî | Sep. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `MarkdownEval` | An image-to-markdown generation benchmark proposed in KOSMOS-2.5, comprising 5,693 document images across categories such as mathematical equations, academic papers, tables, general documents, and project documentation. It evaluates structural fidelity and textual accuracy using normalized edit distance and normalized tree edit distance. | ‚Äî | Sep. 2023 |
| <a href="https://link.springer.com/article/10.1007/s11432-024-4235-6"><img src="./figs/SC-logo.png" width="80"></a> | `OCRBench` | A comprehensive OCR evaluation benchmark for large multimodal models, covering 29 datasets across diverse text-related visual tasks including text recognition, scene text VQA, document VQA, key information extraction, and handwritten mathematical expression recognition. It evaluates multilingual, handwritten, non-semantic, and formula-level text understanding to systematically assess OCR capabilities in the LMM era. | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MultimodalOCR) | May 2023 |

## üìÑ Specialized Model

### üìÑ Document Dewarping
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_ForCenNet_Foreground-Centric_Network_for_Document_Image_Rectification_ICCV_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `ForCenNet` | Qihoo 360 Technology | ForCenNet: Foreground-Centric Network for Document Image Rectification | [![GitHub Stars](https://img.shields.io/github/stars/caipeng328/ForCenNet?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/caipeng328/ForCenNet) | Oct. 2025 |
| <a href="https://openaccess.thecvf.com/content/CVPR2025W/WiCV/papers/Kumari_Document_Image_Rectification_using_Stable_Diffusion_Transformer_CVPRW_2025_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `Kumari & Das` | IIT Madras | Document Image Rectification using Stable Diffusion Transformer | --- | Jun. 2025 |
| <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Hertlein_DocMatcher_Document_Image_Dewarping_via_Structural_and_Textual_Line_Matching_WACV_2025_paper.pdf"><img src="./figs/WACV-logo.png" width="80"></a> | `DocMatcher` | FZI & KIT | DocMatcher: Document Image Dewarping via Structural and Textual Line Matching | [![GitHub Stars](https://img.shields.io/github/stars/FelixHertlein/doc-matcher?style=for-the-badge&logo=github&label=GitHub&color=black)](https://felixhertlein.github.io/doc-matcher) | Mar. 2025 |
| <a href="https://link.springer.com/article/10.1007/s11263-025-02431-5"><img src="./figs/IJCV-logo.png" width="80"></a> | `DocScanner` | USTC | DocScanner: Robust Document Image Rectification with Progressive Learning | [![GitHub Stars](https://img.shields.io/github/stars/fh2019ustc/DocScanner?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/fh2019ustc/DocScanner) | Jan. 2025 |
| <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_DocRes_A_Generalist_Model_Toward_Unifying_Document_Image_Restoration_Tasks_CVPR_2024_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `DocRes` | SCUT | DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks | [![GitHub Stars](https://img.shields.io/github/stars/ZZZHANG-jx/DocRes?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ZZZHANG-jx/DocRes) | Jun. 2024 |
| <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28366"><img src="./figs/AAAI-logo.png" width="80"></a> | `DocNLC` | SCUT | DocNLC: A Document Image Enhancement Framework with Normalized and Latent Contrastive Representation for Multiple Degradations | [![GitHub Stars](https://img.shields.io/github/stars/RylonW/DocNLC?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/RylonW/DocNLC) | Feb. 2024 |
| <a href="https://dl.acm.org/doi/10.1145/3664647.3681548"><img src="./figs/ACM-MM-logo.jpg" width="80"></a> | `Document Registration` | XJTLU | Document Registration: Towards Automated Labeling of Pixel-Level Alignment Between Warped-Flat Documents | --- | Jan. 2024 |
| <a href="https://dl.acm.org/doi/fullHtml/10.1145/3610548.3618174"><img src="./figs/SIGGRAPH-logo.jpg" width="80"></a> | `UVDoc` | ETH Zurich | UVDoc: Neural Grid-based Document Unwarping | [![GitHub Stars](https://img.shields.io/github/stars/tanguymagne/UVDoc?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/tanguymagne/UVDoc) | Oct. 2023 |
| <a href="https://openaccess.thecvf.com/content/ICCV2023W/NIVT/papers/Hertlein_Template-Guided_Illumination_Correction_for_Document_Images_with_Imperfect_Geometric_Reconstruction_ICCVW_2023_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `IIITrTemplate` | FZI | Template-guided illumination correction for document images with imperfect geometric reconstruction | [![GitHub Stars](https://img.shields.io/github/stars/FelixHertlein/illtrtemplate-model?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/FelixHertlein/illtrtemplate-model) | Oct. 2023 |
| <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Foreground_and_Text-lines_Aware_Document_Image_Rectification_ICCV_2023_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `Foreground and Text-lines Aware Model` | Harbin Institute of Technology Shenzhen, China | Foreground and text-lines aware document image rectification | [![GitHub Stars](https://img.shields.io/github/stars/xiaomore/Document-Image-Dewarping?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/xiaomore/Document-Image-Dewarping) | Jun. 2023 |
### üìÑ Physical Structure Analysis
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.17213) | `PP-DocLayout` | PaddlePaddle Team, Baidu | PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleX?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleX) | Mar. 2025 |
| <a href="https://link.springer.com/article/10.1007/s10032-025-00556-4"><img src="./figs/IJDAR-logo.png" width="80"></a> | `DCEM-ViT` | Sharda University, Greater Noida, India | Devanagari character encoded mix-merge vision transformer for robust document layout analysis | --- | Mar. 2025 |
| <a href="https://link.springer.com/chapter/10.1007/978-3-032-04627-7_28"><img src="./figs/ICDAR-logo.png" width="80"></a> | `HiLEx` | Institute of Engineering and Management, Kolkata, India | HiLEx: Image-Based Hierarchical Layout Extraction from Question Papers | [![GitHub Stars](https://img.shields.io/github/stars/HiLEX-DLA/HiLEX?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/HiLEX-DLA/HiLEX) | Mar. 2025 |
| <a href="https://openaccess.thecvf.com/content/ICCV2025W/VisionDocs/papers/Shehzadi_DocSemi_Efficient_Document_Layout_Analysis_with_Guided_Queries_ICCVW_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `DocSemi` |  Technical University of Kaiserslautern, Germany | DocSemi: Efficient Document Layout Analysis with Guided Queries | --- | Feb. 2025 |
| <a href="https://openaccess.thecvf.com/content/ICCV2025W/WiCV/papers/Shehzadi_Efficient_Additive_Attention_for_Transformer-based_Semi-supervised_Document_Layout_Analysis_ICCVW_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `Efficient Additive Attention DLA` | Technical University of Kaiserslautern, Germany | Efficient Additive Attention for Transformer-based Semi-supervised Document Layout Analysis | --- | Feb. 2025 |
| <a href="https://doi.org/10.1007/s10032-025-00539-5"><img src="./figs/IJDAR-logo.png" width="80"></a> | `FS-QCSNet` | China University of Mining and Technology | Few-Shot Quaternion-valued Correlation Squeeze Network for Document Image Layout Segmentation | --- | Jan. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2410.12628) | `DocLayout-YOLO` | OpenDataLab | DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/DocLayout-YOLO?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/DocLayout-YOLO) | Oct. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_LayoutLLM_Layout_Instruction_Tuning_with_Large_Language_Models_for_Document_CVPR_2024_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `LayoutLLM` | Alibaba Research | LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM) | Jun. 2024 |
| <a href="https://aclanthology.org/2024.acl-long.463.pdf"><img src="./figs/ACL-logo.png" width="80"></a> | `DocLLM` | JPMorgan AI Research | DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/dswang2011/DocLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dswang2011/DocLLM) | May 2024 |
| <a href="https://link.springer.com/article/10.1007/s10032-024-00473-y"><img src="./figs/IJDAR-logo.png" width="80"></a> | `SemiDocSeg` | Computer Vision Center, Barcelona, Spain | SemiDocSeg: Harnessing Semi-Supervised Learning for Document Layout Analysis | --- | Mar. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_GeoLayoutLM_Geometric_Pre-Training_for_Visual_Information_Extraction_CVPR_2023_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `GeoLayoutLM` | Alibaba Research | GeoLayoutLM: Geometric Pre-training for Visual Information Extraction | [![GitHub Stars](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/GeoLayoutLM) | Jun. 2023 |
| <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548112"><img src="./figs/ACM-MM-logo.jpg" width="80"></a> | `LayoutLMv3` | Microsoft | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/unilm?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/unilm/tree/master/layoutlmv3) | Oct. 2022 |
### üìÑ Reading Order Prediction
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.10258) | `XY-cut++` | Tianjin University | XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a Novel Benchmark | - | Apr. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](http://arxiv.org/abs/2409.19672) | `ROOR` | Fudan University | Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/chongzhangFDU/ROOR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chongzhangFDU/ROOR) | Sep. 2024 |
| <a href="https://aclanthology.org/2023.emnlp-main.846/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `Reading Order Matters` | Fudan University | Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction | [![GitHub Stars](https://img.shields.io/github/stars/chongzhangFDU/Token-Path-Prediction-Datasets?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chongzhangFDU/Token-Path-Prediction-Datasets) | Dec. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2108.11591) | `LayoutReader` | University of California, San Diego+Microsoft Research Asia| LayoutReader: Pre-training of Text and Layout for Reading Order Detection | - | Aug. 2021 |
| <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548112"><img src="./figs/ACM-MM-logo.jpg" width="80"></a> | `LayoutLMv3` | Sun Yat-sen University | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/unilm?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/unilm/tree/master/layoutlmv3) | Oct. 2022 |
### üìÑ Mathematical Expression Recognition
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Geometry Problem-solving
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Table Recognition
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Chart Understanding
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Scene Text Spotting
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2602.04030) | `TiCLS` | University of Minnesota | TiCLS: Tightly Coupled Language Text Spotter | [![GitHub Stars](https://img.shields.io/github/stars/knowledge-computing/TiCLS?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/knowledge-computing/TiCLS) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.09966) | `SemiETS` | Huazhong University of Science and Technology | SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/DrLuo/SemiETS?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/DrLuo/SemiETS) | Apr. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2412.15523) | `InstructOCR` | Meituan | InstructOCR: Instruction Boosting Scene Text Spotting | - | Dec. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.14998) | `FastTextSpotter` | Indian Statistical Institute | FastTextSpotter: A High-Efficiency Transformer for Multilingual Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/alloydas/Fast-Textspotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alloydas/Fast-Textspotter) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.00355) | `DNTextSpotter` | Soochow University | DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training | [![GitHub Stars](https://img.shields.io/github/stars/yyyyyxie/DNTextSpotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/yyyyyxie/DNTextSpotter) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2404.00852) | `Ensemble Learning` | University of Science, VNU-HCM | Ensemble Learning for Vietnamese Scene Text Spotting in Urban Environments | - | Apr. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2404.04624) | `Bridging Text Spotting` | South China University of Technology | EBridging the Gap Between End-to-End and Two-Step Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/mxin262/Bridging-Text-Spotting?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mxin262/Bridging-Text-Spotting) | Apr. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2403.10047) | `TextBlockV2` | Chinese Academy of Sciences | TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model | - | Mar. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2402.17134) | `Linguistic Priors` | University of Rochester | Efficiently Leveraging Linguistic Priors for Scene Text Spotting | - | Feb. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2401.07641) | `SwinTextSpotter v2` | South China University of Technology | SwinTextSpotter v2: Towards Better Synergy for Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/mxin262/SwinTextSpotterv2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mxin262/SwinTextSpotterv2) | Jan. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2401.03637) | `IATS` | University of Science and Technology Beijing | Inverse-like Antagonistic Scene Text Spotting via Reading-Order Estimation and Dynamic Sampling | - | Jan. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2312.15690) | `WordLenSpotter` | Wuhan Institute of Technology | Word length-aware text spotting: Enhancing detection and recognition in dense text image | - | Dec. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.02356) | `STEP` | Computer Vision Center, UAB | STEP -- Towards Structured Scene-Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/CVC-DAG/STEP?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/CVC-DAG/STEP) | Sep. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.10147) | `ESTextSpotter` | South China University of Technology | ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy in Transformer | [![GitHub Stars](https://img.shields.io/github/stars/mxin262/ESTextSpotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mxin262/ESTextSpotter) | Aug. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2304.03435) | `UNITS` | Naver Cloud | Towards Unified Scene Text Spotting based on Sequence Generation | [![GitHub Stars](https://img.shields.io/github/stars/clovaai/units?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/clovaai/units) | Apr. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2302.10641) | `A3S` | Fast Accounting Co., Ltd. | A3S: Adversarial learning of semantic representations for Scene-Text Spotting | - | Feb. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2301.01635) | `SPTS v2` | Huazhong University of Science and Technology | SPTS v2: Single-Point Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/SPTSv2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/SPTSv2) | Jan. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2211.10578) | `ABINet++` | University of Science and Technology of China | ABINet++: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/FangShancheng/ABINet-PP?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/FangShancheng/ABINet-PP) | Nov. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2211.10772) | `DeepSolo` | Wuhan University | DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/ViTAE-Transformer/DeepSolo?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ViTAE-Transformer/DeepSolo) | Nov. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://dl.acm.org/doi/pdf/10.1145/3503161.3547882) | `TPSNet` | Institute of Information Engineering | TPSNet: Reverse Thinking of Thin Plate Splines for Arbitrary Shape Scene Text Representation | [![GitHub Stars](https://img.shields.io/github/stars/Wei-ucas/TPSNet?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Wei-ucas/TPSNet) | Nov. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2208.03364) | `GLASS` | Technion | GLASS: Global to Local Attention for Scene-Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/amazon-science/glass-text-spotting?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/amazon-science/glass-text-spotting) | Aug. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2203.10209) | `SwinTextSpotter` | South China University of Technology | SwinTextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition | [![GitHub Stars](https://img.shields.io/github/stars/mxin262/SwinTextSpotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mxin262/SwinTextSpotter) | Mar. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2203.05122) | `DEER` | Naver Clova | DEER: Detection-agnostic End-to-End Recognizer for Scene Text Spotting | - | Mar. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2112.07917) | `SPTS` | South China University of Technology | SPTS: Single-Point Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/shannanyinxiang/SPTS?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/shannanyinxiang/SPTS) | Dec. 2021 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2105.00405) | `PAN++` | Nanjing University | PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text | [![GitHub Stars](https://img.shields.io/github/stars/whai362/pan_pp.pytorch?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whai362/pan_pp.pytorch) | May. 2021 |    
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2105.03620) | `ABCNet v2` | South China University of Technology | ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/aim-uofa/AdelaiDet?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/aim-uofa/AdelaiDet) | May. 2021 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2012.04350) | `MANGO` | Hikvision Research Institute | MANGO: A Mask Attention Guided One-Stage Scene Text Spotter | [![GitHub Stars](https://img.shields.io/github/stars/hikopensource/DAVAR-Lab-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/hikopensource/DAVAR-Lab-OCR) | Dec. 2020 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2008.00714) | `AE TextSpotter` | Nanjing University | AE TextSpotter: Learning Visual and Linguistic Representation for Ambiguous Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/whai362/AE_TextSpotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whai362/AE_TextSpotter) | Aug. 2020 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2007.09482) | `Mask TextSpotter v3` | Huazhong University of Science and Technology | Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/MhLiao/MaskTextSpotterV3?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/MhLiao/MaskTextSpotterV3) | Jul. 2020 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://aaai.org/ojs/index.php/AAAI/article/view/6864/6718) | `Text perceptron` | Hikvision Research Institute | Text perceptron: Towards end-to-end arbitrary-shaped text spotting | [![GitHub Stars](https://img.shields.io/github/stars/hikopensource/DAVAR-Lab-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/hikopensource/DAVAR-Lab-OCR) | Apr. 2020 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2002.10200) | `ABCNet` | South China University of Technology | ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network | [![GitHub Stars](https://img.shields.io/github/stars/aim-uofa/AdelaiDet?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/aim-uofa/AdelaiDet) | Feb. 2020 |


## Contributing
We welcome contributions from the community and encourage pull requests to help keep this project up to date. Please do not hesitate to contact us or open an issue
 if you need any assistance.
