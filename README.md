## Overview
A curated survey of OCR in the era of large language models, covering visual text parsing, understanding, benchmarks, challenges, and perspective. 


## üéâ News
- **[2026-2-11]** üî• We release an open-source resource to help the community easily track recent OCR research!

## üìñ Contents
- [Overview](#overview)
- [üéâ News](#-news)
- [üìñ Contents](#-contents)
- [üìÑ Visual Text Parsing](#-visual-text-parsing)
- [üìÑ Visual Text Understanding](#-visual-text-understanding)
- [üìÑ Benchmarks and Evaluation](#-benchmarks-and-evaluation)
- [üìÑ Specialized Model](#-specialized-model)
  - [üìÑ Document Dewarping](#-document-dewarping)
  - [üìÑ Physical Structure Analysis](#-physical-structure-analysis)
  - [üìÑ Reading Order Prediction](#-reading-order-prediction)
  - [üìÑ Mathematical Expression Recognition](#-mathematical-expression-recognition)
  - [üìÑ Table Understanding](#-table-understanding)
  - [üìÑ Table Recognition](#-table-recognition)
  - [üìÑ Chart Understanding](#-chart-understanding)
  - [üìÑ Scene Text Spotting](#-scene-text-spotting)
- [Contributing](#contributing)

## üìÑ Visual Text Parsing
| Venue | Name | Primary affiliation | Title | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| - | `GLM-OCR` | Z.ai | - | [![GitHub Stars](https://img.shields.io/github/stars/zai-org/GLM-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zai-org/GLM-OCR) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2602.05384) | `Dolphin-2.0` | ByteDance | Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.20552) | `DeepSeek-OCR 2` | DeepSeek-AI | DeepSeek-OCR 2: Visual Causal Flow | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR-2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-OCR-2) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.21639) | `OCRVerse` | Meituan | OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models | [![GitHub Stars](https://img.shields.io/github/stars/DocTron-hub/OCRVerse?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/DocTron-hub/OCRVerse) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.20430) | `Youtu-Parsing` | Tencent | Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding | [![GitHub Stars](https://img.shields.io/github/stars/TencentCloudADP/youtu-parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/TencentCloudADP/youtu-parsing) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.21957) | `PaddleOCR-VL 1.5` | Baidu Inc. | PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.15098) | `Uni-Parser` | DP Technology | Uni-Parser Technical Report | [![GitHub](https://img.shields.io/github/stars/uni-parser/uni-parser.github.io?style=for-the-badge&logo=github&label=GitHub&color=black)](https://uni-parser.github.io/) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.01248) | `TRivia` | The University of HongKong | TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognitio | [![GitHub](https://img.shields.io/github/stars/opendatalab/TRivia?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/TRivia) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.10619) | `DOCR-Inspector` | Peking University | DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM| [![GitHub](https://img.shields.io/github/stars/ZZZZZQT/DOCR-Inspector?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ZZZZZQT/DOCR-Inspector) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.21095) | `UniRec-0.1B` | Fudan University | UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters| [![GitHub](https://img.shields.io/github/stars/Topdu/OpenOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Topdu/OpenOCR) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2511.19575) | `HunyuanOCR` | Tencent | HunyuanOCR Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/Tencent-Hunyuan/HunyuanOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Tencent-Hunyuan/HunyuanOCR) | Nov. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.10390) | `MonkeyOCR v1.5` | KingSoft Office | MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.19817) | `OLMOCR 2` | Allen Institute for AI | olmOCR 2: Unit Test Rewards for Document OCR | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Oct. 2025 |
| ‚Äî | `Chandra v0.1.0` | Datalab | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/chandra?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/chandra) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.18234) | `DeepSeek-OCR` | DeepSeek-AI | DeepSeek-OCR: Contexts Optical Compression | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-OCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.03197) | `Infinity-Parser` | INFLY Tech | Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/infly-ai/INF-MLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/infly-ai/INF-MLLM) | Oct. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://nanonets.com/research/nanonets-ocr-2/) | `Nanonets-OCR 2` | Nanonets | Transforming documents into LLM-ready structured data | [![GitHub Stars](https://img.shields.io/github/stars/NanoNets/docstrange?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/NanoNets/docstrange) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.14528) | `PaddleOCR-VL` | Baidu Inc. | PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact VLM | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2505.14059) | `Dolphin-1.5` | ByteDance | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.19760) | `Logics-Parsing` | Alibaba Group | Logics-Parsing Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Logics-Parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/Logics-Parsing) | Sep. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://huggingface.co/ibm-granite/granite-docling-258M) | `Granite-Docling-258M` | IBM | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) | Sep. 2025 |
| ‚Äî | `Marker 1.10.1` | Datalab | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.22186) | `MinerU 2.5` | Shanghai Artificial Intelligence Laboratory | MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Res Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2508.13238?) | `DianJin-OCR-R1` | Qwen DianJin Team | DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model | [![GitHub Stars](https://img.shields.io/github/stars/aliyun/qwen-dianjin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/aliyun/qwen-dianjin) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.02498)| `dots.ocr` | Xiaohongshu Inc. | dots.ocr: Multilingual Document Layout Parsing in a Single VLM | [![GitHub Stars](https://img.shields.io/github/stars/rednote-hilab/dots.ocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/rednote-hilab/dots.ocr) | Jul. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | `MonkeyOCR-Pro` | HUST | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Jul. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://ocrflux.pdfparser.io/#/blog) | `OCRFlux` | ChatDOC | OCRFlux: Mastering Complex Layouts and Seamless Page Merging | [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| ‚Äî | `MinerU2.0-2505-0.9B` | Shanghai Artificial Intelligence Laboratory | | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | `MonkeyOCR` | HUST | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Jun. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1130//"><img src="./figs/ACL-logo.png" width="80"></a> | `Dolphin` | ByteDance | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | May. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2506.07553)| `GTR-VL` | Shanghai Artificial Intelligence Laborator | GTR-VL: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition | - | May. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.11576) | `SmolDocling-256M` | IBM | SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) | Mar. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `OLMOCR` | Allen Institute for AI | olmOCR: Unlocking Trillions of Tokens in PDFs with VLMs | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/html/2507.05595v1) | `PP-StructureV3` | Baidu Inc. | PaddleOCR 3.0 Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2501.15558) | `Ocean-OCR` | Baichuan Inc. | Ocean-OCR: Towards General OCR Application via a Vision-Language Model | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) | Jan. 2025 |
| ‚Äî | `MarkItDown` | Microsoft | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/markitdown?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/markitdown) | Dec. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.01704) | `GOT-OCR 2.0` | StepFun | General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model | [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/GOT-OCR2.0?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/GOT-OCR2.0) | Sep. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.18839) | `MinerU` | Shanghai Artificial Intelligence Laboratory | MinerU: An Open-Source Solution for Precise Document Content Extraction | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Sep. 2024 |
| ‚Äî | `open-parse` | - | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/Filimoa/open-parse?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Filimoa/open-parse) | Mar. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.13418) | `Nougat` | Meta AI | Nougat: Neural Optical Understanding for Academic Documents | [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/nougat?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/facebookresearch/nougat) | Aug. 2023 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://unstructured.io/) | `unstructured` | Unstructured | Transform complex, unstructured data into clean, structured data. Securely. Continuously. Effortlessly. | [![GitHub Stars](https://img.shields.io/github/stars/Unstructured-IO/unstructured?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Unstructured-IO/unstructured) | Oct. 2022 |
| ‚Äî | `RapidOCR` | Unstructured | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/RapidAI/RapidOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/RapidAI/RapidOCR) | Jul. 2022 |


## üìÑ Visual Text Understanding
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.21631)  | `Qwen3-VL`| Alibaba Group |Qwen3-VL Technical Report |[![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/QwenLM/Qwen2.5-VL) |Nov. 2025|
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2508.18265) | `InternVL3.5`|  Shanghai AI Laboratory | InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/InternVL) |Apr. 2025|
| <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DocThinker_Explainable_Multimodal_Large_Language_Models_with_Rule-based_Reinforcement_Learning_ICCV_2025_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"> |`Doc Thinker` |HUST| DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding | --- |  Aug. 2025 |
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.10479) | `InternVL3`|  Shanghai AI Laboratory | InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/InternVL) |Apr. 2025|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.07491) | `Kimi-VL` | MoonshotAI | Kimi-VL: Vision-Language Model for Advanced Multimodal Understanding | [![GitHub Stars](https://img.shields.io/github/stars/MoonshotAI/Kimi-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/MoonshotAI/Kimi-VL) | Apr. 2025 |
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.04065) | `PP-DocBee`|  Baidu | PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleMIX?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleMIX) |Mar. 2025|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.01743) | `Phi-4-Mini` | Microsoft | Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/PhiCookBook?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/PhiCookBook) | Mar. 2025 |
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.16161) | `OmniParser V2`| HUST | OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models |---|Feb. 2025|
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2412.05271) | `InternVL2.5`|  Shanghai AI Laboratory | PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/InternVL) |Dec. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2412.10302) | `DeepSeek-VL2` | DeepSeek AI | DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-VL2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-VL2) | Dec. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2410.07073) | `Pixtral-12B` | Mistral AI | Pixtral 12B: A Vision-Language Model for Advanced Multimodal Understanding | [![GitHub Stars](https://img.shields.io/github/stars/mistralai/mistral-inference?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mistralai/mistral-inference/) | Oct. 2024 |
| <a href="https://aclanthology.org/2025.acl-long.291/"><img src="./figs/ACL-logo.png" width="80"></a> | `mPLUG-DocOwl2` | Alibaba Group | mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2) | Sept. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.17146) | `Molmo & PixMo` | Allen Institute for AI | Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models | [![GitHub Stars](https://img.shields.io/github/stars/allenai/molmo?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/molmo) | Sep. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liao_DocLayLLM_An_Efficient_Multi-modal_Extension_of_Large_Language_Models_for_CVPR_2025_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"> |`DocLayLLM`| South China Univ. of Techn. + Alibaba Cloud|DocLayLLM: An Efficient Multi-modal Extension of Large Language Models for Text-rich Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/whlscut/DocLayLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whlscut/DocLayLLM) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.12637) | `Cog-iDEFICS3` | Hugging Face | Cog-iDEFICS3: Scaling Multimodal Models with Integrated Field-aware Embeddings and Semantic Compositionality | [![GitHub Stars](https://img.shields.io/github/stars/zsxkib/cog-idefics3?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zsxkib/cog-idefics3) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.15998) | `Eagle-X5` | NVIDIA | Eagle-X5: Scalable Vision-Language Models with Enhanced Cross-modal Alignment and Generative Capability | [![GitHub Stars](https://img.shields.io/github/stars/NVlabs/Eagle?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/NVlabs/Eagle) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2406.12793) | `GLM-4` | Zhipu AI | ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools | [![GitHub Stars](https://img.shields.io/github/stars/zai-org/GLM-4?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zai-org/GLM-4) | Jun. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2407.07895) | `LLaVA-NeXT` | ByteDance | LLaVA-NeXT: Next-Generation Large Vision-Language Models with Enhanced Multimodal Capabilities | [![GitHub Stars](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/LLaVA-VL/LLaVA-NeXT) | Jul. 2024 |
| <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/9ee3a664ccfeabc0da16ac6f1f1cfe59-Paper-Conference.pdf"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `Cambrian-1` | New York University | Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal Large Language Models | [![GitHub Stars](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge&logo=github)](https://github.com/cambrian-mllm/cambrian) | Jun. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2403.04473) | `TextMonkey` | HUST | TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/Monkey?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/Monkey) | May. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.20797) | `Ovis` | Alibaba Group | Ovis: Structural Embedding Alignment for Multimodal Large Language Model | [![GitHub Stars](https://img.shields.io/github/stars/AIDC-AI/Ovis?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AIDC-AI/Ovis) | May 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2404.09204)  | `TextHawk` | Zhejiang University|TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models | [![GitHub Stars](https://img.shields.io/github/stars/yuyq96/TextHawk?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/yuyq96/TextHawk) |Apr. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2404.06512) | `InternLM-XComposer2-4KHD` | Shanghai AI Laboratory | InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD | [![GitHub Stars](https://img.shields.io/github/stars/InternLM/InternLM-XComposer?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/InternLM/InternLM-XComposer) | Apr. 2024 |
|  <a href="https://aclanthology.org/2024.acl-long.463/"><img src="./figs/ACL-logo.png" width="80"></a> | `DocLLM` | JPMorgan AI Research | DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding|[![GitHub Stars](https://img.shields.io/github/stars/dswang2011/DocLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dswang2011/DocLLM) |  Jan. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.14295)  | `Fox` | University of Chinese Academy of Sciences | Focus Anywhere for Fine-grained Multi-page Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/ucaslcl/Fox?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ucaslcl/Fox) |May. 2024|
| <a href="https://aclanthology.org/2024.findings-emnlp.175/"><img src="./figs/EMNLP-logo.png" width="80"> |`mPLUG-DocOwl 1.5`| Alibaba Group | mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5) |Mar. 2024|
| <a href="https://eccv.ecva.net/virtual/2024/poster/2237"><img src="./figs/ECCV-logo.svg" width="80"></a> | `Vary`| MEGVII Technology |  Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models |  [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/Vary?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/Vary) |Dec. 2023|
| <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Monkey_Image_Resolution_and_Text_Label_Are_Important_Things_for_CVPR_2024_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"> |`Monkey`| HUST |Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models | [![GitHub Stars](https://img.shields.io/github/stars/whlscut/DocLayLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whlscut/DocLayLLM) | Nov. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `KOSMOS-2.5`|Microsoft | KOSMOS-2.5: A Multimodal Literate Model| [![GitHub Stars](https://img.shields.io/github/stars/microsoft/unilm?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/unilm/tree/master/kosmos-2.5) | Sept. 2023|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.12966) | `QwenVL` | Alibaba Group | QwenVL: A Vision-Language Model for Unified Visual and Language Understanding | [![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/QwenLM/Qwen-VL) | Aug. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2307.02499) |  `mPLUG-DocOwl` | DAMO Academy, Alibaba Group |mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding |  [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl) |Jul. 2023|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2306.01733) | `DocFormerv2` | AWS AI Labs |DocFormerv2: Local Features for Document Understanding |---| Jun. 2023|

## üìÑ Benchmarks and Evaluation

| Venue | Benchmark Name | Description  | Link | Date |
|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2512.02498) | `dots.ocr-bench` | A multilingual end-to-end document parsing benchmark introduced to measure cross-lingual generalization beyond EN/ZH-centric benchmarks, constructed from real-world documents spanning 126 languages; the paper states the benchmark will be released and reports evaluation using edit-distance and table TEDS style metrics. | ‚Äî | Dec. 2025 |
| <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CC-OCR_A_Comprehensive_and_Challenging_OCR_Benchmark_for_Evaluating_Large_ICCV_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `CC-OCR` | A comprehensive OCR-centric benchmark for evaluating large multimodal models across four tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. It contains 39 subsets with 7,058 fully annotated images, 41% sourced from real-world applications, designed to assess fine-grained literacy, text grounding, multi-orientation robustness, and hallucination behavior. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/wulipc/CC-OCR) [![GitHub Stars](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Benchmarks/CC-OCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.19760) | `LogicsParsingBench` | A page-level document parsing benchmark introduced for evaluating complex layout analysis and reading order inference, consisting of 1,078 PDF page images spanning 9 major document categories and more than 20 sub-categories, including challenging layouts such as multi-column newspapers, posters, chemical formulas, and handwritten Chinese text. | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Logics-Parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/Logics-Parsing) | Sept. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1128/"><img src="./figs/ACL-logo.png" width="80"></a> | `READoc` | A unified benchmark for realistic document structured extraction that defines DSE as converting unstructured PDFs into semantically rich Markdown. The dataset contains 3,576 real-world documents sourced from arXiv, GitHub, and Zenodo, and is paired with an evaluation suite for standardization, segmentation, and scoring to support unified comparison across DSE systems. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/lazyc/READoc) [![GitHub Stars](https://img.shields.io/github/stars/icip-cas/READoc?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/icip-cas/READoc) | Jul. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1130/"><img src="./figs/ACL-logo.png" width="80"></a> | `Dolphin-Page` | A bilingual benchmark of 210 document pages designed for complex document image parsing and natural reading order reconstruction; it includes 111 pure-text documents and additional pages with mixed/complex layouts (e.g., tables, formulas, figures) to stress layout analysis and structured extraction. | [![GitHub Stars](https://img.shields.io/github/stars/ByteDance/Dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ByteDance/Dolphin) | Jul. 2025 |
| <a href="https://cvpr.thecvf.com/virtual/2025/poster/34400"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `OmniDocBench (v1.5)` | A document understanding benchmark with 1,355 PDF pages and over 100,000 fine-grained annotations (70k+ span-level, 20k+ block-level), balancing Chinese/English page ratio and increasing resolution for diverse document types, designed to evaluate OCR and structured extraction robustness. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/opendatalab/OmniDocBench/tree/main) [![GitHub](https://img.shields.io/github/stars/opendatalab/OmniDocBench?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/OmniDocBench) | Jun. 2025 |
| ‚Äî | `OCRFlux-bench-single` | A single-page PDF-to-Markdown benchmark containing 2000 bilingual (EN/ZH) PDF pages with manually verified ground-truth Markdown, designed to evaluate complex layout parsing including multi-column text, tables, figures, and formulas. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-single) [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| ‚Äî | `OCRFlux-bench-cross` | A cross-page PDF-to-Markdown benchmark with 1000 bilingual (EN/ZH) samples, where each sample consists of Markdown element sequences from two consecutive pages and annotated index pairs indicating elements that should be merged, targeting evaluation of cross-page table and paragraph merging. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-cross) [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.03197) | `Infinity-Doc-55K` | A large-scale scanned document parsing benchmark combining 55K high-fidelity synthetic pages with expert-filtered real-world documents, designed to evaluate layout-aware parsing including OCR accuracy, table and formula extraction, paragraph segmentation, and reading order preservation across English and Chinese documents. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/infly/Infinity-Doc-55K) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `olmOCR-bench` | A unit-test style OCR evaluation benchmark with 1,403 PDF documents and 7,010 machine-verifiable pass/fail test cases, designed to stress challenging document phenomena such as formulas, tables, tiny fonts, old scans, and other layout and structure preservation issues in PDF-to-text extraction. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/allenai/olmOCR-bench) [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `olmOCR-mix-0225` | A large-scale PDF-to-text training dataset consisting of 260,000 pages sampled from over 100,000 crawled PDFs with diverse layouts and quality levels, including graphics, handwritten text, and low-quality scans, used to fine-tune a 7B vision-language model for structured, natural reading-order PDF extraction. | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2501.15558) | `Ocean-OCR-OCR_eval_data` | An evaluation suite for OCR practical scenarios covering (1) bilingual dense document extraction with 100 English paper images and 100 Chinese paper images, (2) scene text recognition with images sampled from MSRA-TD500 and manually verified ground truth (bootstrapped from PaddleOCR), and (3) multi-granularity bilingual handwritten recognition (real and synthetic; paragraph- and line-level), with 100 samples per category. | [![GitHub Stars](https://img.shields.io/github/stars/guoxy25/Ocean-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/guoxy25/Ocean-OCR) | Jan. 2025 |
| ‚Äî | `marker_benchmark` | An overall PDF-to-Markdown evaluation set built from single PDF pages extracted from Common Crawl, paired with block-level ground truth segments for scoring. The released dataset contains 2,138 page samples with fields including rendered page image and ground-truth blocks, and is used in Marker‚Äôs ‚ÄúOverall PDF Conversion‚Äù benchmark with heuristic alignment and LLM-judge scoring. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/datalab-to/marker_benchmark) [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) | Jan. 2025 |
| <a href="https://openreview.net/forum?id=Vb6i3Dp24N"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `OCRBench_v2` | A large-scale bilingual text-centric benchmark for evaluating LMM OCR capabilities beyond recognition, covering visual text localization and reasoning across 23 tasks and 31 diverse scenarios, with 10,000 human-verified question-answer pairs and a high proportion of difficult samples. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ling99/OCRBench_v2) [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MultimodalOCR) | Dec. 2024 |
| <a href="https://aclanthology.org/2024.findings-emnlp.175/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `DocLocal4K` | A text localization and recognition evaluation benchmark for OCR-free document understanding, containing 4,250 document image samples balanced across four granularities (word, phrase, line, block). It evaluates both text grounding (IOU@0.5) and text recognition (BLEU1‚ÄìBLEU4) to assess spatial preservation and fine-grained localization capability in document images. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/mPLUG/DocLocal4K) [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5) | Nov. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2410.21311) | `MMDocBench` | An OCR-free document understanding benchmark for evaluating fine-grained visual perception and reasoning in large vision-language models. It defines 15 tasks with 4,338 QA pairs and 11,353 supporting regions across diverse document types including research papers, receipts, financial reports, tables, charts, and infographics. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/next-tat/MMDocBench) [![GitHub Stars](https://img.shields.io/github/stars/fengbinzhu/MMDocBench?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/fengbinzhu/MMDocBench) | Oct. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.14295) | `Fox-benchmark` | A bilingual fine-grained multi-page document understanding benchmark with 9 sub-tasks (page OCR, line-level OCR, color-guided OCR, region-level OCR/translation/summary, multi-page multi-region OCR, cross-page VQA). It contains 112 English pages and 100 Chinese pages (single/multi-column; >1,000 words per page), plus an additional 200 rendered interleaved pages for in-document figure caption evaluation. | [![GitHub Stars](https://img.shields.io/github/stars/ucaslcl/Fox?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ucaslcl/Fox) | May 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `OCREval` | A document-level OCR benchmark introduced with KOSMOS-2.5, consisting of 2,297 images collected from the test sets of 13 datasets, covering mathematical content, handwritten text, design images, receipts, digitally born documents, and web pages. It evaluates document-level text recognition using F1, IOU, and normalized edit distance metrics. | ‚Äî | Sep. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `MarkdownEval` | An image-to-markdown generation benchmark proposed in KOSMOS-2.5, comprising 5,693 document images across categories such as mathematical equations, academic papers, tables, general documents, and project documentation. It evaluates structural fidelity and textual accuracy using normalized edit distance and normalized tree edit distance. | ‚Äî | Sep. 2023 |
| <a href="https://link.springer.com/article/10.1007/s11432-024-4235-6"><img src="./figs/SC-logo.png" width="80"></a> | `OCRBench` | A comprehensive OCR evaluation benchmark for large multimodal models, covering 29 datasets across diverse text-related visual tasks including text recognition, scene text VQA, document VQA, key information extraction, and handwritten mathematical expression recognition. It evaluates multilingual, handwritten, non-semantic, and formula-level text understanding to systematically assess OCR capabilities in the LMM era. | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MultimodalOCR) | May 2023 |

## üìÑ Specialized Model

### üìÑ Document Dewarping
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Physical Structure Analysis
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Reading Order Prediction
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Mathematical Expression Recognition
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Table Understanding
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Table Recognition
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Chart Understanding
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|

### üìÑ Scene Text Spotting
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|



## Contributing
We welcome contributions from the community and encourage pull requests to help keep this project up to date. Please do not hesitate to contact us or open an issue
 if you need any assistance.
