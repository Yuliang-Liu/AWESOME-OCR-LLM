## Overview
A curated survey of OCR in the era of large language models, covering visual text parsing, understanding, benchmarks, challenges, and perspective. Paper is coming soon. 


## üéâ News
- **[2026-2-11]** üî• We release an open-source resource to help the community easily track recent OCR research!

## üìñ Contents
- [News](#-news)
- [Visual Text Parsing](#-visual-text-parsing)
- [Visual Text Understanding](#-visual-text-understanding)
- [Benchmarks and Evaluation](#-benchmarks-and-evaluation)

## üìÑ Visual Text Parsing
##### End-to-end
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.20552) | `DeepSeek-OCR 2` | DeepSeek-AI | DeepSeek-OCR 2: Visual Causal Flow | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR-2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-OCR-2) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.21639) | `OCRVerse` | Meituan | OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models | [![GitHub Stars](https://img.shields.io/github/stars/DocTron-hub/OCRVerse?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/DocTron-hub/OCRVerse) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2511.19575) | `HunyuanOCR` | Tencent | HunyuanOCR Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/Tencent-Hunyuan/HunyuanOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Tencent-Hunyuan/HunyuanOCR) | Nov. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.19817) | `OLMOCR 2` | Allen Institute for AI | olmOCR 2: Unit Test Rewards for Document OCR | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Oct. 2025 |
| ‚Äî | `Chandra v0.1.0` | Datalab | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/chandra?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/chandra) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.18234) | `DeepSeek-OCR` | DeepSeek-AI | DeepSeek-OCR: Contexts Optical Compression | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-OCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.03197) | `Infinity-Parser` | INFLY Tech | Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/infly-ai/INF-MLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/infly-ai/INF-MLLM) | Oct. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://nanonets.com/research/nanonets-ocr-2/) | `Nanonets-OCR 2` | Nanonets | Transforming documents into LLM-ready structured data | [![GitHub Stars](https://img.shields.io/github/stars/NanoNets/docstrange?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/NanoNets/docstrange) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.19760) | `Logics-Parsing` | Alibaba Group | Logics-Parsing Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Logics-Parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/Logics-Parsing) | Sep. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://huggingface.co/ibm-granite/granite-docling-258M) | `Granite-Docling-258M` | IBM | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.02498)| `dots.ocr` | Xiaohongshu Inc. | dots.ocr: Multilingual Document Layout Parsing in a Single VLM | [![GitHub Stars](https://img.shields.io/github/stars/rednote-hilab/dots.ocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/rednote-hilab/dots.ocr) | Jul. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://ocrflux.pdfparser.io/#/blog) | `OCRFlux` | ChatDOC | OCRFlux: Mastering Complex Layouts and Seamless Page Merging | [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| ‚Äî | `MinerU2.0-2505-0.9B` |  Shanghai Artificial Intelligence Laboratory | | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.11576) | `SmolDocling-256M` | IBM | SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) | Mar. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `OLMOCR` | Allen Institute for AI | olmOCR: Unlocking Trillions of Tokens in PDFs with VLMs | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2501.15558) | `Ocean-OCR` | Baichuan Inc. | Ocean-OCR: Towards General OCR Application via a Vision-Language Model | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) | Jan. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.01704) | `GOT-OCR 2.0` | StepFun | General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model | [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/GOT-OCR2.0?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/GOT-OCR2.0) | Sep. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.13418) | `Nougat` | Meta AI | Nougat: Neural Optical Understanding for Academic Documents | [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/nougat?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/facebookresearch/nougat) | Aug. 2023 |
##### Pipeline
| Venue | Name | Primary affiliation | Title | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| - | `GLM-OCR` | Z.ai | - | [![GitHub Stars](https://img.shields.io/github/stars/zai-org/GLM-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zai-org/GLM-OCR) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2602.05384) | `Dolphin-2.0` | ByteDance | Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.20430) | `Youtu-Parsing` | Tencent | Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding | [![GitHub Stars](https://img.shields.io/github/stars/TencentCloudADP/youtu-parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/TencentCloudADP/youtu-parsing) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.21957) | `PaddleOCR-VL 1.5` | Baidu Inc. | PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.14528) | `PaddleOCR-VL` | Baidu Inc. | PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact VLM | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2505.14059) | `Dolphin-1.5` | ByteDance | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | Oct. 2025 |
| ‚Äî | `Marker 1.10.1` | Datalab | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.22186) | `MinerU 2.5` | Shanghai Artificial Intelligence Laboratory | MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Res Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | `MonkeyOCR-Pro` | HUST | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Jul. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | `MonkeyOCR` | HUST | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Jun. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1130//"><img src="./figs/ACL-logo.png" width="80"></a> | `Dolphin` | ByteDance | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | May. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/html/2507.05595v1) | `PP-StructureV3` | Baidu Inc. | PaddleOCR 3.0 Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Feb. 2025 |
| ‚Äî | `MarkItDown` | Microsoft | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/markitdown?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/markitdown) | Dec. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.18839) | `MinerU` | Shanghai Artificial Intelligence Laboratory | MinerU: An Open-Source Solution for Precise Document Content Extraction | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Sep. 2024 |
| ‚Äî | `open-parse` | - | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/Filimoa/open-parse?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Filimoa/open-parse) | Mar. 2024 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://unstructured.io/) | `unstructured` | Unstructured | Transform complex, unstructured data into clean, structured data. Securely. Continuously. Effortlessly. | [![GitHub Stars](https://img.shields.io/github/stars/Unstructured-IO/unstructured?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Unstructured-IO/unstructured) | Oct. 2022 |
| ‚Äî | `RapidOCR` | Unstructured | ‚Äî | [![GitHub Stars](https://img.shields.io/github/stars/RapidAI/RapidOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/RapidAI/RapidOCR) | Jul. 2022 |


## üìÑ Visual Text Understanding
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| <a href="https://aclanthology.org/2025.acl-long.291/"><img src="./figs/ACL-logo.png" width="80"></a> | `mPLUG-DocOwl2` | Alibaba Group | mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2) | Sept. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.14295)  | `Fox` | University of Chinese Academy of Sciences | Focus Anywhere for Fine-grained Multi-page Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/ucaslcl/Fox?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ucaslcl/Fox) |May. 2024|
| <a href="https://aclanthology.org/2024.findings-emnlp.175/"><img src="./figs/EMNLP-logo.png" width="80"> |`mPLUG-DocOwl 1.5`| Alibaba Group | mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5) |Mar. 2024|
| <a href="https://eccv.ecva.net/virtual/2024/poster/2237"><img src="./figs/ECCV-logo.svg" width="80"></a> | `Vary`| MEGVII Technology |  Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models |  [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/Vary?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/Vary) |Dec. 2023|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `KOSMOS-2.5`|Microsoft | KOSMOS-2.5: A Multimodal Literate Model| [![GitHub Stars](https://img.shields.io/github/stars/microsoft/unilm?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/unilm/tree/master/kosmos-2.5) | Sept. 2023|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2307.02499) |  `mPLUG-DocOwl` | DAMO Academy, Alibaba Group |mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding |  [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl) |Jul. 2023|

## üìÑ Benchmarks and Evaluation

| Venue | Benchmark Name | Description  | Link | Date |
|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2512.02498) | `dots.ocr` | A multilingual end-to-end document parsing benchmark introduced to measure cross-lingual generalization beyond EN/ZH-centric benchmarks, constructed from real-world documents spanning 126 languages; the paper states the benchmark will be released and reports evaluation using edit-distance and table TEDS style metrics. | [![GitHub Stars](https://img.shields.io/github/stars/rednote-hilab/dots.ocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/rednote-hilab/dots.ocr) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.19760) | `LogicsParsingBench` | A page-level document parsing benchmark introduced for evaluating complex layout analysis and reading order inference, consisting of 1,078 PDF page images spanning 9 major document categories and more than 20 sub-categories, including challenging layouts such as multi-column newspapers, posters, chemical formulas, and handwritten Chinese text. | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Logics-Parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/Logics-Parsing) | Sept. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1128/"><img src="./figs/ACL-logo.png" width="80"></a> | `READoc` | A unified benchmark for realistic document structured extraction that defines DSE as converting unstructured PDFs into semantically rich Markdown. The dataset contains 3,576 real-world documents sourced from arXiv, GitHub, and Zenodo, and is paired with an evaluation suite for standardization, segmentation, and scoring to support unified comparison across DSE systems. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/lazyc/READoc) [![GitHub Stars](https://img.shields.io/github/stars/icip-cas/READoc?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/icip-cas/READoc) | Jul. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1130/"><img src="./figs/ACL-logo.png" width="80"></a> | `Dolphin-Page` | A bilingual benchmark of 210 document pages designed for complex document image parsing and natural reading order reconstruction; it includes 111 pure-text documents and additional pages with mixed/complex layouts (e.g., tables, formulas, figures) to stress layout analysis and structured extraction. | [![GitHub Stars](https://img.shields.io/github/stars/ByteDance/Dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ByteDance/Dolphin) | Jul. 2025 |
| <a href="https://cvpr.thecvf.com/virtual/2025/poster/34400"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `OmniDocBench (v1.5)` | A document understanding benchmark with 1,355 PDF pages and over 100,000 fine-grained annotations (70k+ span-level, 20k+ block-level), balancing Chinese/English page ratio and increasing resolution for diverse document types, designed to evaluate OCR and structured extraction robustness. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/opendatalab/OmniDocBench/tree/main) [![GitHub](https://img.shields.io/github/stars/opendatalab/OmniDocBench?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/OmniDocBench) | Jun. 2025 |
| ‚Äî | `OCRFlux-bench-single` | A single-page PDF-to-Markdown benchmark containing 2000 bilingual (EN/ZH) PDF pages with manually verified ground-truth Markdown, designed to evaluate complex layout parsing including multi-column text, tables, figures, and formulas. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-single) [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| ‚Äî | `OCRFlux-bench-cross` | A cross-page PDF-to-Markdown benchmark with 1000 bilingual (EN/ZH) samples, where each sample consists of Markdown element sequences from two consecutive pages and annotated index pairs indicating elements that should be merged, targeting evaluation of cross-page table and paragraph merging. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-cross) [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.03197) | `Infinity-Doc-55K` | A large-scale scanned document parsing benchmark combining 55K high-fidelity synthetic pages with expert-filtered real-world documents, designed to evaluate layout-aware parsing including OCR accuracy, table and formula extraction, paragraph segmentation, and reading order preservation across English and Chinese documents. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/infly/Infinity-Doc-55K) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `olmOCR-bench` | A unit-test style OCR evaluation benchmark with 1,403 PDF documents and 7,010 machine-verifiable pass/fail test cases, designed to stress challenging document phenomena such as formulas, tables, tiny fonts, old scans, and other layout and structure preservation issues in PDF-to-text extraction. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/allenai/olmOCR-bench) [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `olmOCR-mix-0225` | A large-scale PDF-to-text training dataset consisting of 260,000 pages sampled from over 100,000 crawled PDFs with diverse layouts and quality levels, including graphics, handwritten text, and low-quality scans, used to fine-tune a 7B vision-language model for structured, natural reading-order PDF extraction. | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2501.15558) | `Ocean-OCR-OCR_eval_data` | An evaluation suite for OCR practical scenarios covering (1) bilingual dense document extraction with 100 English paper images and 100 Chinese paper images, (2) scene text recognition with images sampled from MSRA-TD500 and manually verified ground truth (bootstrapped from PaddleOCR), and (3) multi-granularity bilingual handwritten recognition (real and synthetic; paragraph- and line-level), with 100 samples per category. | [![GitHub Stars](https://img.shields.io/github/stars/guoxy25/Ocean-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/guoxy25/Ocean-OCR) | Jan. 2025 |
| ‚Äî | `marker_benchmark` | An overall PDF-to-Markdown evaluation set built from single PDF pages extracted from Common Crawl, paired with block-level ground truth segments for scoring. The released dataset contains 2,138 page samples with fields including rendered page image and ground-truth blocks, and is used in Marker‚Äôs ‚ÄúOverall PDF Conversion‚Äù benchmark with heuristic alignment and LLM-judge scoring. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/datalab-to/marker_benchmark) [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) | Jan. 2025 |
| <a href="https://openreview.net/forum?id=Vb6i3Dp24N"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `OCRBench_v2` | A large-scale bilingual text-centric benchmark for evaluating LMM OCR capabilities beyond recognition, covering visual text localization and reasoning across 23 tasks and 31 diverse scenarios, with 10,000 human-verified question-answer pairs and a high proportion of difficult samples. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ling99/OCRBench_v2) [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://)_)_




## Contributing
We welcome contributions from the community and encourage pull requests to help keep this project up to date. Please do not hesitate to contact me at ylliu@hust.edu.cn
 if you need any assistance.
