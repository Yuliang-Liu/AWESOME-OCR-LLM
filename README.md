## Overview
A curated survey of OCR in the era of large language models, covering visual text parsing, understanding, benchmarks, challenges, and perspective. 


## ðŸŽ‰ News
- **[2026-2-11]** ðŸ”¥ We release an open-source resource to help the community easily track recent OCR research!

## ðŸ“– Contents
- [Overview](#overview)
- [ðŸŽ‰ News](#-news)
- [ðŸ“– Contents](#-contents)
- [ðŸ“„ Visual Text Parsing](#-visual-text-parsing)
- [ðŸ“„ Visual Text Understanding](#-visual-text-understanding)
- [ðŸ“„ Benchmarks and Evaluation](#-benchmarks-and-evaluation)
- [ðŸ“„ Specialized Model](#-specialized-model)
  - [ðŸ“„ Document Dewarping](#-document-dewarping)
  - [ðŸ“„ Physical Structure Analysis](#-physical-structure-analysis)
  - [ðŸ“„ Reading Order Prediction](#-reading-order-prediction)
  - [ðŸ“„ Mathematical Expression Recognition](#-mathematical-expression-recognition)
  - [ðŸ“„ Geometry Problem-solving](#-geometry-problem-solving)
  - [ðŸ“„ Chart Understanding](#-chart-understanding)
  - [ðŸ“„ Scene Text Spotting](#-scene-text-spotting)
- [Contributing](#contributing)

## ðŸ“„ Visual Text Parsing
| Venue | Name | Primary affiliation | Title | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2602.06402) | `MeDocVL` | Ping An Property | MeDocVL: A Visual Language Model for Medical Document Understanding and Parsing | [![GitHub Stars](https://img.shields.io/github/stars/Dejavuvvw/MeDocVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Dejavuvvw/MeDocVL)  | Feb. 2026 |
| - | `GLM-OCR` | Z.ai | - | [![GitHub Stars](https://img.shields.io/github/stars/zai-org/GLM-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zai-org/GLM-OCR) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2602.05384) | `Dolphin-2.0` | ByteDance | Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.20552) | `DeepSeek-OCR 2` | DeepSeek-AI | DeepSeek-OCR 2: Visual Causal Flow | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR-2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-OCR-2) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2601.14722) | `Typhoon-OCR` | Typhoon, SCB 10X | Typhoon OCR: Open Vision-Language Model for Thai Document Extraction | [![GitHub Stars](https://img.shields.io/github/stars/scb-10x/typhoon-ocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/scb-10x/typhoon-ocr) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2601.14490) | `GutenOCR` | Roots.ai | GutenOCR: A Grounded Vision-Language Front-End for Documents | [![GitHub Stars](https://img.shields.io/github/stars/Roots-Automation/GutenOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Roots-Automation/GutenOCR) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.21639) | `OCRVerse` | Meituan | OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models | [![GitHub Stars](https://img.shields.io/github/stars/DocTron-hub/OCRVerse?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/DocTron-hub/OCRVerse) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.20430) | `Youtu-Parsing` | Tencent | Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding | [![GitHub Stars](https://img.shields.io/github/stars/TencentCloudADP/youtu-parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/TencentCloudADP/youtu-parsing) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.21957) | `PaddleOCR-VL 1.5` | Baidu Inc. | PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2601.14251) | `LightOnOCR` | LightOn | LightOnOCR: A 1B END-TO-END MULTILINGUAL VISION-LANGUAGE MODEL FOR STATE-OF-THE-ART OCR | [![GitHub Stars](https://img.shields.io/badge/HuggingFace-Link-yellow)](https://huggingface.co/collections/lightonai/lightonocr-2) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.15098) | `Uni-Parser` | DP Technology | Uni-Parser Technical Report | [![GitHub](https://img.shields.io/github/stars/uni-parser/uni-parser.github.io?style=for-the-badge&logo=github&label=GitHub&color=black)](https://uni-parser.github.io/) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.01248) | `TRivia` | The University of HongKong | TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognitio | [![GitHub](https://img.shields.io/github/stars/opendatalab/TRivia?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/TRivia) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.10619) | `DOCR-Inspector` | Peking University | DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM| [![GitHub](https://img.shields.io/github/stars/ZZZZZQT/DOCR-Inspector?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ZZZZZQT/DOCR-Inspector) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.21095) | `UniRec-0.1B` | Fudan University | UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters| [![GitHub](https://img.shields.io/github/stars/Topdu/OpenOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Topdu/OpenOCR) | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2511.19575) | `HunyuanOCR` | Tencent | HunyuanOCR Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/Tencent-Hunyuan/HunyuanOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Tencent-Hunyuan/HunyuanOCR) | Nov. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.10390) | `MonkeyOCR v1.5` | KingSoft Office | MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Nov. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2510.21603) | `Doc-Researcher` | Huawei | Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research |  | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.19817) | `OLMOCR 2` | Allen Institute for AI | olmOCR 2: Unit Test Rewards for Document OCR | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Oct. 2025 |
| â€” | `Chandra v0.1.0` | Datalab | â€” | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/chandra?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/chandra) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.18234) | `DeepSeek-OCR` | DeepSeek-AI | DeepSeek-OCR: Contexts Optical Compression | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-OCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.03197) | `Infinity-Parser` | INFLY Tech | Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/infly-ai/INF-MLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/infly-ai/INF-MLLM) | Oct. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://nanonets.com/research/nanonets-ocr-2/) | `Nanonets-OCR 2` | Nanonets | Transforming documents into LLM-ready structured data | [![GitHub Stars](https://img.shields.io/github/stars/NanoNets/docstrange?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/NanoNets/docstrange) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.14528) | `PaddleOCR-VL` | Baidu Inc. | PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact VLM | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2505.14059) | `Dolphin-1.5` | ByteDance | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.19760) | `Logics-Parsing` | Alibaba Group | Logics-Parsing Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Logics-Parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/Logics-Parsing) | Sep. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://huggingface.co/ibm-granite/granite-docling-258M) | `Granite-Docling-258M` | IBM | â€” | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) | Sep. 2025 |
| â€” | `Marker 1.10.1` | Datalab | â€” | [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.22186) | `MinerU 2.5` | Shanghai Artificial Intelligence Laboratory | MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Res Document Parsing | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2508.13238?) | `DianJin-OCR-R1` | Qwen DianJin Team | DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model | [![GitHub Stars](https://img.shields.io/github/stars/aliyun/qwen-dianjin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/aliyun/qwen-dianjin) | Sep. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2512.02498)| `dots.ocr` | Xiaohongshu Inc. | dots.ocr: Multilingual Document Layout Parsing in a Single VLM | [![GitHub Stars](https://img.shields.io/github/stars/rednote-hilab/dots.ocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/rednote-hilab/dots.ocr) | Jul. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | `MonkeyOCR-Pro` | HUST | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Jul. 2025 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://ocrflux.pdfparser.io/#/blog) | `OCRFlux` | ChatDOC | OCRFlux: Mastering Complex Layouts and Seamless Page Merging | [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| â€” | `MinerU2.0-2505-0.9B` | Shanghai Artificial Intelligence Laboratory | | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | `MonkeyOCR` | HUST | MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MonkeyOCR) | Jun. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1130//"><img src="./figs/ACL-logo.png" width="80"></a> | `Dolphin` | ByteDance | Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting | [![GitHub Stars](https://img.shields.io/github/stars/bytedance/dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/bytedance/dolphin) | May. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2506.07553)| `GTR-VL` | Shanghai Artificial Intelligence Laborator | GTR-VL: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition | - | May. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.11576) | `SmolDocling-256M` | IBM | SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion | [![GitHub Stars](https://img.shields.io/github/stars/docling-project/docling?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/docling-project/docling) | Mar. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `OLMOCR` | Allen Institute for AI | olmOCR: Unlocking Trillions of Tokens in PDFs with VLMs | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/html/2507.05595v1) | `PP-StructureV3` | Baidu Inc. | PaddleOCR 3.0 Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleOCR) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2501.15558) | `Ocean-OCR` | Baichuan Inc. | Ocean-OCR: Towards General OCR Application via a Vision-Language Model | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) | Jan. 2025 |
| â€” | `MarkItDown` | Microsoft | â€” | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/markitdown?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/markitdown) | Dec. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.01704) | `GOT-OCR 2.0` | StepFun | General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model | [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/GOT-OCR2.0?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/GOT-OCR2.0) | Sep. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.18839) | `MinerU` | Shanghai Artificial Intelligence Laboratory | MinerU: An Open-Source Solution for Precise Document Content Extraction | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/MinerU?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/MinerU) | Sep. 2024 |
| â€” | `open-parse` | - | â€” | [![GitHub Stars](https://img.shields.io/github/stars/Filimoa/open-parse?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Filimoa/open-parse) | Mar. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.13418) | `Nougat` | Meta AI | Nougat: Neural Optical Understanding for Academic Documents | [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/nougat?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/facebookresearch/nougat) | Aug. 2023 |
| [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://unstructured.io/) | `unstructured` | Unstructured | Transform complex, unstructured data into clean, structured data. Securely. Continuously. Effortlessly. | [![GitHub Stars](https://img.shields.io/github/stars/Unstructured-IO/unstructured?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Unstructured-IO/unstructured) | Oct. 2022 |
| â€” | `RapidOCR` | Unstructured | â€” | [![GitHub Stars](https://img.shields.io/github/stars/RapidAI/RapidOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/RapidAI/RapidOCR) | Jul. 2022 |


## ðŸ“„ Visual Text Understanding
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2601.05163)  | `DocDancer` | Peking University | DocDancer: Towards Agentic Document-Grounded Information Seeking | --- | Jan. 2026 |
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.11552)  | `DocLens`| Peking University | DocLens: A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/dwzhu-pku/DocLens?style=for-the-badge&logo=github&label=GitHub&color=black)](https://dwzhu-pku.github.io/DocLens/) |Nov. 2025|
|  <a href="https://arxiv.org/abs/2511.10552"><img src="./figs/AAAI-logo.png" width="80">  | `URaG`| South China Univ. of Tech. | URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/shi-yx/URaG?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/shi-yx/URaG) |Nov. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.21631)  | `Qwen3-VL`| Alibaba Group |Qwen3-VL Technical Report |[![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/QwenLM/Qwen2.5-VL) |Nov. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.03929)  | `NVIDIA Nemotron Nano V2 VL`| NVIDIA | NVIDIA Nemotron Nano V2 VL | [![GitHub Stars](https://img.shields.io/badge/HuggingFace-Link-yellow)](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16) |Nov. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2510.14979)  | `NEO`| Nanyang Technological Univ. | FROM PIXELS TO WORDS â€“ TOWARDS NATIVE VISIONLANGUAGE PRIMITIVES AT SCALE | [![GitHub Stars](https://img.shields.io/github/stars/EvolvingLMMs-Lab/NEO?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/EvolvingLMMs-Lab/NEO) |Oct. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2510.11496)  | `AndesVL`| OPPO | AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model | [![GitHub Stars](https://img.shields.io/github/stars/OPPO-Mente-Lab/AndesVL_Evaluation?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OPPO-Mente-Lab/AndesVL_Evaluation) |Oct. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.23661)  | `LLaVA-OneVision-1.5`| LLaVA Community | LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training | [![GitHub Stars](https://img.shields.io/github/stars/EvolvingLMMs-Lab/LLaVA-OneVision-1.5?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5) |Sept. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.17765)  | `Qwen3-Omni`| Alibaba Group | Qwen3-Omni Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen3-Omni?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/QwenLM/Qwen3-Omni) |Sept. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.14033)  | `SAIL-VL2`| ByteDance | SAIL-VL2 Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/BytedanceDouyinContent/SAIL-VL2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/BytedanceDouyinContent/SAIL-VL2) |Sept. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.18154)  | `MiniCPM-V 4.5`| OpenBMB | MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipes | [![GitHub Stars](https://img.shields.io/github/stars/OpenBMB/MiniCPM-o?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenBMB/MiniCPM-o) |Sept. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.13067)  | `HERO`| Fudan University | HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models | --- |Sept. 2025|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2508.18265) | `InternVL3.5`|  Shanghai AI Laboratory | InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/InternVL) |Aug. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2508.11737)  | `Ovis2.5`| Alibaba | Ovis2.5 Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/AIDC-AI/Ovis?style=for-the-badge&logo=github&label=GitHub&color=black)](http://github.com/AIDC-AI/Ovis) |Aug. 2025|
| <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DocThinker_Explainable_Multimodal_Large_Language_Models_with_Rule-based_Reinforcement_Learning_ICCV_2025_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"> |`Doc Thinker` |HUST| DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding | --- |  Aug. 2025 |
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2507.22058)  | `X-Omni`| Tencent | X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again | [![GitHub Stars](https://img.shields.io/github/stars/X-Omni-Team/X-Omni?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-Omni-Team/X-Omni) |Jul. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2507.06261)  | `Gemini 2.5 Pro`| Google | Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context... | --- |Jul. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2507.19891v1)  | `WeThink-Qwen2.5 VL`| Tencent | WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning | [![GitHub Stars](https://img.shields.io/github/stars/yangjie-cv/WeThink?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/yangjie-cv/WeThink) |Jul. 2025|
| <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Janus_Decoupling_Visual_Encoding_for_Unified_Multimodal_Understanding_and_Generation_CVPR_2025_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `Janus` | DeepSeek-AI | Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/Janus) | Jun. 2025 |
|  [![Blog](https://img.shields.io/badge/blog-0A66C2?style=for-the-badge)](https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6)  | `Seed1.6-vision`| ByteDance | - | --- |Jun. 2025|
|  <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_SparseMM_Head_Sparsity_Emerges_from_Visual_Concept_Responses_in_MLLMs_ICCV_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a>  | `SparseMM`| Tsinghua University | SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs | [![GitHub Stars](https://img.shields.io/github/stars/CR400AF-A/SparseMM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/CR400AF-A/SparseMM) |Jun. 2025|
|  <a href="https://openreview.net/pdf?id=NEBa0bs5LR"><img src="./figs/ICML-logo.png" width="80"></a>  | `DS-VLM`| Xiamen University | DS-VLM: Diffusion Supervision Vision Language Model | --- |Jun. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2505.07062)  | `Seed1.5-VL`| ByteDance | Seed1.5-VL Technical Report | --- |May. 2025|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.10479) | `InternVL3`|  Shanghai AI Laboratory | InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/InternVL) |Apr. 2025|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.07491) | `Kimi-VL` | MoonshotAI | Kimi-VL: Vision-Language Model for Advanced Multimodal Understanding | [![GitHub Stars](https://img.shields.io/github/stars/MoonshotAI/Kimi-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/MoonshotAI/Kimi-VL) | Apr. 2025 |
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.10462)  | `SAIL-VL`| Bytedance | The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer | [![GitHub Stars](https://img.shields.io/github/stars/ByteDance-Seed/SAIL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ByteDance-Seed/SAIL) |Apr. 2025|
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.04065) | `PP-DocBee`|  Baidu | PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleMIX?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleMIX) |Mar. 2025|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.01743) | `Phi-4-Mini` | Microsoft | Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/PhiCookBook?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/PhiCookBook) | Mar. 2025|
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.16161) | `OmniParser V2`| HUST | OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models |---|Feb. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.14420)  | `ChatVLA`| Midea Group | ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model | [![GitHub Stars](https://img.shields.io/github/stars/tutujingyugang1/ChatVLA_public?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/tutujingyugang1/ChatVLA_public) |Feb. 2025|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.13923)  | `Qwen2.5-VL`| Alibaba | Qwen2.5-VL Technical Report | [![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen3-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/QwenLM/Qwen3-VL) |Feb. 2025|
|  -  | `MiniCPM-o-2.6`| OpenBMB | MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech, and Multimodal Live Streaming on Your Phone | [![GitHub Stars](https://img.shields.io/github/stars/OpenBMB/MiniCPM-o?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenBMB/MiniCPM-o) |Jan. 2025|
|[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2412.05271) | `InternVL2.5`|  Shanghai AI Laboratory | PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/InternVL) |Dec. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2412.10302) | `DeepSeek-VL2` | DeepSeek AI | DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-VL2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-VL2) | Dec. 2024 |
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2403.05530)  | `Gemini 1.5 Pro`| Google | Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context | --- |Dec. 2024|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2411.04952)  | `M3DOCRAG`| UNC Chapel Hill | M3DOCRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/Omaralsaabi/M3DOCRAG?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Omaralsaabi/M3DOCRAG) |Nov. 2024|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2411.01106)  | `SV-RAG`| University at Buffalo | SV-RAG: LORA-CONTEXTUALIZING ADAPTATION OF MLLMS FOR LONG DOCUMENT UNDERSTANDING | [![GitHub Stars](https://img.shields.io/github/stars/puar-playground/Self-Visual-RAG?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/puar-playground/Self-Visual-RAG) |Nov. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2410.07073) | `Pixtral-12B` | Mistral AI | Pixtral 12B: A Vision-Language Model for Advanced Multimodal Understanding | [![GitHub Stars](https://img.shields.io/github/stars/mistralai/mistral-inference?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mistralai/mistral-inference/) | Oct. 2024 |
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2410.05970)  | `PDF-WuKong`| HUST | PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling | [![GitHub Stars](https://img.shields.io/github/stars/yh-hust/PDF-Wukong?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/yh-hust/PDF-Wukong) |Oct. 2024|
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2410.10594)  | `VISRAG`| Tsinghua University | VISRAG: VISION-BASED RETRIEVAL-AUGMENTED GENERATION ON MULTI-MODALITY DOCUMENTS | [![GitHub Stars](https://img.shields.io/github/stars/openbmb/visrag?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/openbmb/visrag) |Oct. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.17146) | `Molmo & PixMo` | Allen Institute for AI | Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models | [![GitHub Stars](https://img.shields.io/github/stars/allenai/molmo?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/molmo) | Sep. 2024 |
|  [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2409.12191)  | `Qwen2-VL`| Alibaba | Qwen2-VL: Enhancing Vision-Language Modelâ€™s Perception of the World at Any Resolution | [![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen3-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/QwenLM/Qwen3-VL) |Sep. 2024|
| <a href="https://arxiv.org/abs/2409.03420"><img src="./figs/ACL-logo.png" width="80"></a> | `mPLUG-DocOwl2` | Alibaba Group | mPLUG-DocOwl2: High-resolution Multi-turn Chat on Doc-images with High-compression OCR-free Visual Features | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl) | Sep. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liao_DocLayLLM_An_Efficient_Multi-modal_Extension_of_Large_Language_Models_for_CVPR_2025_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"> |`DocLayLLM`| South China Univ. of Techn. + Alibaba Cloud|DocLayLLM: An Efficient Multi-modal Extension of Large Language Models for Text-rich Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/whlscut/DocLayLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whlscut/DocLayLLM) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.12637) | `Cog-iDEFICS3` | Hugging Face | Cog-iDEFICS3: Scaling Multimodal Models with Integrated Field-aware Embeddings and Semantic Compositionality | [![GitHub Stars](https://img.shields.io/github/stars/zsxkib/cog-idefics3?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zsxkib/cog-idefics3) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.15998) | `Eagle-X5` | NVIDIA | Eagle-X5: Scalable Vision-Language Models with Enhanced Cross-modal Alignment and Generative Capability | [![GitHub Stars](https://img.shields.io/github/stars/NVlabs/Eagle?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/NVlabs/Eagle) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2407.07895) | `LLaVA-NeXT` | ByteDance | LLaVA-NeXT: Next-Generation Large Vision-Language Models with Enhanced Multimodal Capabilities | [![GitHub Stars](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/LLaVA-VL/LLaVA-NeXT) | Jul. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2406.12793) | `GLM-4` | Zhipu AI | ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools | [![GitHub Stars](https://img.shields.io/github/stars/zai-org/GLM-4?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zai-org/GLM-4) | Jun. 2024 |
| <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/9ee3a664ccfeabc0da16ac6f1f1cfe59-Paper-Conference.pdf"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `Cambrian-1` | New York University | Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal Large Language Models | [![GitHub Stars](https://img.shields.io/github/stars/cambrian-mllm/cambrian?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/cambrian-mllm/cambrian) | Jun. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2024/html/EMU2-chat_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `EMU2-chat` | Beijing Academy of Artificial Intelligence | EMU2-chat: Generative Multimodal Models are In-Context Learners | [![GitHub Stars](https://img.shields.io/github/stars/baaivision/Emu?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/baaivision/Emu) | Jun. 2024 |
| <a href="https://arxiv.org/abs/2403.04473"><img src="./figs/TPAMI-logo.jpg" width="80"></a>  | `TextMonkey` | HUST | TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/Monkey?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/Monkey) | May. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.20797) | `Ovis` | Alibaba Group | Ovis: Structural Embedding Alignment for Multimodal Large Language Model | [![GitHub Stars](https://img.shields.io/github/stars/AIDC-AI/Ovis?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AIDC-AI/Ovis) | May 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.14295)  | `Fox` | University of Chinese Academy of Sciences | Focus Anywhere for Fine-grained Multi-page Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/ucaslcl/Fox?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ucaslcl/Fox) |May. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2404.09204)  | `TextHawk` | Zhejiang University|TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models | [![GitHub Stars](https://img.shields.io/github/stars/yuyq96/TextHawk?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/yuyq96/TextHawk) |Apr. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2404.06512) | `InternLM-XComposer2-4KHD` | Shanghai AI Laboratory | InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD | [![GitHub Stars](https://img.shields.io/github/stars/InternLM/InternLM-XComposer?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/InternLM/InternLM-XComposer) | Apr. 2024 |
| <a href="https://arxiv.org/abs/2404.16635"><img src="./figs/EMNLP-logo.png" width="80"></a> | `TinyChart` | Alibaba Group | TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/TinyChart?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart) | Apr. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/html/2403.04652v1) | `Yi-VL` | 01.AI | Yi: Open Foundation Models by 01.AI â€” Vision-Language Model | [![GitHub Stars](https://img.shields.io/github/stars/01-ai/Yi?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/01-ai/Yi) | Mar. 2024 |
| <a href="https://aclanthology.org/2024.findings-emnlp.175/"><img src="./figs/EMNLP-logo.png" width="80"> |`mPLUG-DocOwl 1.5`| Alibaba Group | mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5) |Mar. 2024|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2403.05525) | `DeepSeek-VL` | DeepSeek-AI | DeepSeek-VL: Towards Real-World Vision-Language Understanding | [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/deepseek-ai/DeepSeek-VL) | Mar. 2024 |
|  <a href="https://aclanthology.org/2024.acl-long.463/"><img src="./figs/ACL-logo.png" width="80"></a> | `DocLLM` | JPMorgan AI Research | DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding|[![GitHub Stars](https://img.shields.io/github/stars/dswang2011/DocLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dswang2011/DocLLM) |  Jan. 2024 |
| <a href="https://eccv.ecva.net/virtual/2024/poster/2237"><img src="./figs/ECCV-logo.svg" width="80"></a> | `Vary`| MEGVII Technology |  Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models |  [![GitHub Stars](https://img.shields.io/github/stars/Ucas-HaoranWei/Vary?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ucas-HaoranWei/Vary) |Dec. 2023|
| <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_Monkey_Image_Resolution_and_Text_Label_Are_Important_Things_for_CVPR_2024_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"> |`Monkey`| HUST |Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models | [![GitHub Stars](https://img.shields.io/github/stars/whlscut/DocLayLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whlscut/DocLayLLM) | Nov. 2023 |
| <a href="https://neurips.cc/virtual/2024/poster/96510"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `CogVLM` | ZaiOrg | CogVLM: Visual Expert for Pretrained Language Models | [![GitHub Stars](https://img.shields.io/github/stars/zai-org/CogVLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zai-org/CogVLM) | Nov. 2023 |
| <a href="https://aclanthology.org/2023.findings-emnlp.187/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `UReader` | East China Normal University | UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model | [![GitHub Stars](https://img.shields.io/github/stars/LukeForeverYoung/UReader?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/LukeForeverYoung/UReader) | Nov. 2023 |
|<a href="https://arxiv.org/abs/2311.18248"><img src="./figs/ACM-MM-logo.jpg" width="80"></a> | `PaperOwl` | Alibaba Group | PaperOwl: Scientific Document Analysis with Multimodal Large Language Models | [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl)| Nov. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `KOSMOS-2.5`|Microsoft | KOSMOS-2.5: A Multimodal Literate Model| [![GitHub Stars](https://img.shields.io/github/stars/microsoft/unilm?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/unilm/tree/master/kosmos-2.5) | Sept. 2023|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.12966) | `QwenVL` | Alibaba Group | QwenVL: A Vision-Language Model for Unified Visual and Language Understanding | [![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen-VL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/QwenLM/Qwen-VL) | Aug. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2307.02499) |  `mPLUG-DocOwl` | DAMO Academy, Alibaba Group |mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding |  [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl) |Jul. 2023|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2306.17107) | `LLaVAR` | Georgia Tech | LLaVAR: Enhanced Visual Instruction Tuning for Text-rich Image Understanding | [![GitHub Stars](https://img.shields.io/github/stars/SALT-NLP/LLaVAR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/SALT-NLP/LLaVAR) | Jun. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2306.01733) | `DocFormerv2` | AWS AI Labs |DocFormerv2: Local Features for Document Understanding |---| Jun. 2023|
| <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `LLaVA` | University of Wisconsinâ€“Madison | Visual Instruction Tuning | [![GitHub Stars](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/haotian-liu/LLaVA) | Apr. 2023 |
| <a href="https://arxiv.org/abs/2111.15664"><img src="./figs/ECCV-logo.svg" width="80"></a> | `Donut` | Naver CLOVA | OCR-free Document Understanding Transformer | [![GitHub Stars](https://img.shields.io/github/stars/clovaai/donut?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/clovaai/donut) | Nov. 2021 |
| <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9633153"><img src="./figs/TPAMI-logo.jpg" width="80"></a> | `SMA` | Northwestern Polytechnical University | Structured Multimodal Attentions for TextVQA | [![GitHub Stars](https://img.shields.io/github/stars/ChenyuGAO-CS/SMA?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ChenyuGAO-CS/SMA) | Nov. 2021 |



## ðŸ“„ Benchmarks and Evaluation
| Venue | Benchmark Name | Description  | Link | Date |
|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.05218) | `MonkeyDoc` | A large-scale document parsing dataset containing 4.5 million bilingual instances across more than ten document types, designed under the Structure-Recognition-Relation paradigm to support structure detection, content recognition, and relation prediction in unified end-to-end document modeling. | [![ModelScope](https://img.shields.io/badge/ModelScope-dataset-6246EA?style=for-the-badge)](https://www.modelscope.cn/datasets/zenosai/MonkeyDoc) [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MonkeyOCR?style=for-the-badge&logo=github&label=GitHub&color=black&cacheSeconds=3600)](https://github.com/Yuliang-Liu/MonkeyOCR) | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2512.02498) | `dots.ocr-bench` | A multilingual end-to-end document parsing benchmark introduced to measure cross-lingual generalization beyond EN/ZH-centric benchmarks, constructed from real-world documents spanning 126 languages; the paper states the benchmark will be released and reports evaluation using edit-distance and table TEDS style metrics. | â€” | Dec. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2511.18434) | `DocPTBench` | A benchmark for end-to-end photographed document parsing and translation comprising over 1,300 high-resolution real-world captured documents across multiple domains. It includes eight translation scenarios with human-verified annotations for both structural parsing and translation, targeting robustness under geometric distortions and photometric variations. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/topdu/DocPTBench) [![GitHub Stars](https://img.shields.io/github/stars/Topdu/DocPTBench?style=for-the-badge&logo=github&label=GitHub&color=black&cacheSeconds=3600)](https://github.com/Topdu/DocPTBench) | Nov. 2025 |
| <a href="https://aclanthology.org/anthology-files/pdf/findings/2025.findings-emnlp.1318.pdf"><img src="./figs/EMNLP-logo.png" width="80"></a> | `TIU-Bench` | A benchmark for evaluating large multimodal models on text-rich image understanding. It covers diverse tasks that require reading and interpreting dense text content embedded in images, pushing modelsâ€™ abilities beyond simple OCR to include comprehension, reasoning, and contextual extraction from visually complex scenes. | - | Nov. 2025 |
| <a href="https://aclanthology.org/2025.emnlp-main.1172.pdf"><img src="./figs/EMNLP-logo.png" width="80"></a> | `WildDoc` | A benchmark for comprehensive and robust document understanding â€œin the wildâ€, evaluating models across diverse real-world document scenarios. WildDoc is designed to stress test OCR, layout extraction, reasoning, and semantic parsing under uncontrolled, noisy, and heterogeneous document conditions. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ByteDance/WildDoc) [![GitHub Stars](https://img.shields.io/github/stars/bytedance/WildDoc?style=for-the-badge&logo=github&label=GitHub&color=black&cacheSeconds=3600)](https://github.com/bytedance/WildDoc) | Nov. 2025 |
| <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Yang_CC-OCR_A_Comprehensive_and_Challenging_OCR_Benchmark_for_Evaluating_Large_ICCV_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `CC-OCR` | A comprehensive OCR-centric benchmark for evaluating large multimodal models across four tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. It contains 39 subsets with 7,058 fully annotated images, 41% sourced from real-world applications, designed to assess fine-grained literacy, text grounding, multi-orientation robustness, and hallucination behavior. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/wulipc/CC-OCR) [![GitHub Stars](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Benchmarks/CC-OCR) | Oct. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.19760) | `LogicsParsingBench` | A page-level document parsing benchmark introduced for evaluating complex layout analysis and reading order inference, consisting of 1,078 PDF page images spanning 9 major document categories and more than 20 sub-categories, including challenging layouts such as multi-column newspapers, posters, chemical formulas, and handwritten Chinese text. | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/Logics-Parsing?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/Logics-Parsing) | Sept. 2025 |
| â€” | `OpenDoc-Null-6K` | An Image-to-Text dataset curated for OCR and scanned document parsing research, containing 6,910 document-style training images in ImageFolder format. It provides raw document images without associated labels, suitable for OCR pretraining, PDF image understanding, and layout/text extraction tasks. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/prithivMLmods/OpenDoc-Null-6K) | Sept. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1128/"><img src="./figs/ACL-logo.png" width="80"></a> | `READoc` | A unified benchmark for realistic document structured extraction that defines DSE as converting unstructured PDFs into semantically rich Markdown. The dataset contains 3,576 real-world documents sourced from arXiv, GitHub, and Zenodo, and is paired with an evaluation suite for standardization, segmentation, and scoring to support unified comparison across DSE systems. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/lazyc/READoc) [![GitHub Stars](https://img.shields.io/github/stars/icip-cas/READoc?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/icip-cas/READoc) | Jul. 2025 |
| <a href="https://icdar2025.org/"><img src="./figs/ICDAR-logo.png" width="80"></a> | `PublicAffairs-Layout-GNN-Bench` | A benchmark for fine-grained layout classification of text blocks in digital-born PDF documents from public affairs sources, comprising 37K PDFs with 441K pages collected from over 20 official sources. It evaluates Graph Neural Network architectures under different graph constructions and multimodal feature fusion strategies for document layout analysis. | --- | Jul. 2025 |
| â€” | `OpenDoc-Pdf-Preview` | A compact Image-to-Text visual preview dataset containing 6,000 high-resolution document images extracted from PDFs, designed for document OCR pretraining, layout understanding, and multimodal document analysis. The dataset is English-only and released in Parquet format for efficient large-scale training. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/prithivMLmods/OpenDoc-Pdf-Preview) | Jul. 2025 |
| <a href="https://aclanthology.org/2025.findings-acl.1130/"><img src="./figs/ACL-logo.png" width="80"></a> | `Dolphin-Page` | A bilingual benchmark of 210 document pages designed for complex document image parsing and natural reading order reconstruction; it includes 111 pure-text documents and additional pages with mixed/complex layouts (e.g., tables, formulas, figures) to stress layout analysis and structured extraction. | [![GitHub Stars](https://img.shields.io/github/stars/ByteDance/Dolphin?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ByteDance/Dolphin) | Jul. 2025 |
| <a href="https://aclanthology.org/2025.acl-long.XXX/"><img src="./figs/ACL-logo.png" width="80"></a> | `LongDocURL` | A comprehensive multimodal long document benchmark integrating understanding, reasoning, and locating abilities for large vision-language models. It evaluates models on complex long-form documents with multimodal content, requiring not only answer generation but also precise evidence localization across extended contexts. | [![GitHub Stars](https://img.shields.io/github/stars/dengc2023/LongDocURL?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dengc2023/LongDocURL) | Jul. 2025 |
| <a href="https://cvpr.thecvf.com/virtual/2025/poster/34400"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `OmniDocBench (v1.5)` | A document understanding benchmark with 1,355 PDF pages and over 100,000 fine-grained annotations (70k+ span-level, 20k+ block-level), balancing Chinese/English page ratio and increasing resolution for diverse document types, designed to evaluate OCR and structured extraction robustness. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/opendatalab/OmniDocBench/tree/main) [![GitHub](https://img.shields.io/github/stars/opendatalab/OmniDocBench?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/OmniDocBench) | Jun. 2025 |
| <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Docopilot_Improving_Multimodal_Models_for_Document-Level_Understanding_CVPR_2025_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `Doc750K` | A large-scale document-level multimodal understanding dataset designed for complex multi-page document comprehension, featuring diverse document structures, extensive cross-page dependencies, and real question-answer pairs grounded in original documents. It supports evaluation of coherence, accuracy, and multi-turn reasoning in document-level multimodal models without relying on retrieval-augmented pipelines. | [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/Docopilot?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OpenGVLab/Docopilot) | Jun. 2025 |
| â€” | `OCRFlux-bench-single` | A single-page PDF-to-Markdown benchmark containing 2000 bilingual (EN/ZH) PDF pages with manually verified ground-truth Markdown, designed to evaluate complex layout parsing including multi-column text, tables, figures, and formulas. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-single) [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| â€” | `OCRFlux-bench-cross` | A cross-page PDF-to-Markdown benchmark with 1000 bilingual (EN/ZH) samples, where each sample consists of Markdown element sequences from two consecutive pages and annotated index pairs indicating elements that should be merged, targeting evaluation of cross-page table and paragraph merging. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ChatDOC/OCRFlux-bench-cross) [![GitHub Stars](https://img.shields.io/github/stars/chatdoc-com/OCRFlux?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chatdoc-com/OCRFlux) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2506.03197) | `Infinity-Doc-55K` | A large-scale scanned document parsing benchmark combining 55K high-fidelity synthetic pages with expert-filtered real-world documents, designed to evaluate layout-aware parsing including OCR accuracy, table and formula extraction, paragraph segmentation, and reading order preservation across English and Chinese documents. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/infly/Infinity-Doc-55K) | Jun. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2505.00063) | `GDI-Bench` | A General Document Intelligence benchmark comprising 2.3k document images across 9 scenarios and 19 document-specific tasks, designed to decouple visual complexity and reasoning complexity for graded difficulty evaluation. It enables fine-grained analysis of model weaknesses in both visual perception and reasoning within document understanding. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/GDIBench) | May 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `olmOCR-bench` | A unit-test style OCR evaluation benchmark with 1,403 PDF documents and 7,010 machine-verifiable pass/fail test cases, designed to stress challenging document phenomena such as formulas, tables, tiny fonts, old scans, and other layout and structure preservation issues in PDF-to-text extraction. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/allenai/olmOCR-bench) [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2502.18443) | `olmOCR-mix-0225` | A large-scale PDF-to-text training dataset consisting of 260,000 pages sampled from over 100,000 crawled PDFs with diverse layouts and quality levels, including graphics, handwritten text, and low-quality scans, used to fine-tune a 7B vision-language model for structured, natural reading-order PDF extraction. | [![GitHub Stars](https://img.shields.io/github/stars/allenai/olmocr?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/allenai/olmocr) | Feb. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2501.15558) | `Ocean-OCR-OCR_eval_data` | An evaluation suite for OCR practical scenarios covering (1) bilingual dense document extraction with 100 English paper images and 100 Chinese paper images, (2) scene text recognition with images sampled from MSRA-TD500 and manually verified ground truth (bootstrapped from PaddleOCR), and (3) multi-granularity bilingual handwritten recognition (real and synthetic; paragraph- and line-level), with 100 samples per category. | [![GitHub Stars](https://img.shields.io/github/stars/guoxy25/Ocean-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guoxy25/Ocean-OCR) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/guoxy25/Ocean-OCR) | Jan. 2025 |
| â€” | `marker_benchmark` | An overall PDF-to-Markdown evaluation set built from single PDF pages extracted from Common Crawl, paired with block-level ground truth segments for scoring. The released dataset contains 2,138 page samples with fields including rendered page image and ground-truth blocks, and is used in Markerâ€™s â€œOverall PDF Conversionâ€ benchmark with heuristic alignment and LLM-judge scoring. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/datalab-to/marker_benchmark) [![GitHub Stars](https://img.shields.io/github/stars/datalab-to/marker?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/datalab-to/marker) | Jan. 2025 |
| <a href="https://openreview.net/forum?id=Vb6i3Dp24N"><img src="./figs/NeurIPS-logo.png" width="80"></a> | `OCRBench_v2` | A large-scale bilingual text-centric benchmark for evaluating LMM OCR capabilities beyond recognition, covering visual text localization and reasoning across 23 tasks and 31 diverse scenarios, with 10,000 human-verified question-answer pairs and a high proportion of difficult samples. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/ling99/OCRBench_v2) [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MultimodalOCR) | Dec. 2024 |
| <a href="https://aclanthology.org/2024.findings-emnlp.175/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `DocLocal4K` | A text localization and recognition evaluation benchmark for OCR-free document understanding, containing 4,250 document image samples balanced across four granularities (word, phrase, line, block). It evaluates both text grounding (IOU@0.5) and text recognition (BLEU1â€“BLEU4) to assess spatial preservation and fine-grained localization capability in document images. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/mPLUG/DocLocal4K) [![GitHub Stars](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5) | Nov. 2024 |
| <a href="https://aclanthology.org/2024.emnlp-main.XXX/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `M-LongDoc` | A multimodal super-long document understanding benchmark with 851 samples drawn from recent documents spanning hundreds of pages, requiring open-ended explanations rather than purely extractive answers. It evaluates large multimodal models under retrieval-aware settings for long-context multimodal document reasoning. | â€” | Nov. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2410.21311) | `MMDocBench` | An OCR-free document understanding benchmark for evaluating fine-grained visual perception and reasoning in large vision-language models. It defines 15 tasks with 4,338 QA pairs and 11,353 supporting regions across diverse document types including research papers, receipts, financial reports, tables, charts, and infographics. | [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-yellow)](https://huggingface.co/datasets/next-tat/MMDocBench) [![GitHub Stars](https://img.shields.io/github/stars/fengbinzhu/MMDocBench?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/fengbinzhu/MMDocBench) | Oct. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2405.14295) | `Fox-benchmark` | A bilingual fine-grained multi-page document understanding benchmark with 9 sub-tasks (page OCR, line-level OCR, color-guided OCR, region-level OCR/translation/summary, multi-page multi-region OCR, cross-page VQA). It contains 112 English pages and 100 Chinese pages (single/multi-column; >1,000 words per page), plus an additional 200 rendered interleaved pages for in-document figure caption evaluation. | [![GitHub Stars](https://img.shields.io/github/stars/ucaslcl/Fox?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ucaslcl/Fox) | May 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `OCREval` | A document-level OCR benchmark introduced with KOSMOS-2.5, consisting of 2,297 images collected from the test sets of 13 datasets, covering mathematical content, handwritten text, design images, receipts, digitally born documents, and web pages. It evaluates document-level text recognition using F1, IOU, and normalized edit distance metrics. | â€” | Sep. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.11419) | `MarkdownEval` | An image-to-markdown generation benchmark proposed in KOSMOS-2.5, comprising 5,693 document images across categories such as mathematical equations, academic papers, tables, general documents, and project documentation. It evaluates structural fidelity and textual accuracy using normalized edit distance and normalized tree edit distance. | â€” | Sep. 2023 |
| <a href="https://link.springer.com/article/10.1007/s11432-024-4235-6"><img src="./figs/SC-logo.png" width="80"></a> | `OCRBench` | A comprehensive OCR evaluation benchmark for large multimodal models, covering 29 datasets across diverse text-related visual tasks including text recognition, scene text VQA, document VQA, key information extraction, and handwritten mathematical expression recognition. It evaluates multilingual, handwritten, non-semantic, and formula-level text understanding to systematically assess OCR capabilities in the LMM era. | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/MultimodalOCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/MultimodalOCR) | May 2023 |

## ðŸ“„ Specialized Model

### ðŸ“„ Document Dewarping
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_ForCenNet_Foreground-Centric_Network_for_Document_Image_Rectification_ICCV_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `ForCenNet` | Qihoo 360 Technology | ForCenNet: Foreground-Centric Network for Document Image Rectification | [![GitHub Stars](https://img.shields.io/github/stars/caipeng328/ForCenNet?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/caipeng328/ForCenNet) | Oct. 2025 |
| <a href="https://openaccess.thecvf.com/content/CVPR2025W/WiCV/papers/Kumari_Document_Image_Rectification_using_Stable_Diffusion_Transformer_CVPRW_2025_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `Kumari & Das` | IIT Madras | Document Image Rectification using Stable Diffusion Transformer | --- | Jun. 2025 |
| <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Hertlein_DocMatcher_Document_Image_Dewarping_via_Structural_and_Textual_Line_Matching_WACV_2025_paper.pdf"><img src="./figs/WACV-logo.png" width="80"></a> | `DocMatcher` | FZI & KIT | DocMatcher: Document Image Dewarping via Structural and Textual Line Matching | [![GitHub Stars](https://img.shields.io/github/stars/FelixHertlein/doc-matcher?style=for-the-badge&logo=github&label=GitHub&color=black)](https://felixhertlein.github.io/doc-matcher) | Mar. 2025 |
| <a href="https://link.springer.com/article/10.1007/s11263-025-02431-5"><img src="./figs/IJCV-logo.png" width="80"></a> | `DocScanner` | USTC | DocScanner: Robust Document Image Rectification with Progressive Learning | [![GitHub Stars](https://img.shields.io/github/stars/fh2019ustc/DocScanner?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/fh2019ustc/DocScanner) | Jan. 2025 |
| <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_DocRes_A_Generalist_Model_Toward_Unifying_Document_Image_Restoration_Tasks_CVPR_2024_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `DocRes` | SCUT | DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks | [![GitHub Stars](https://img.shields.io/github/stars/ZZZHANG-jx/DocRes?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ZZZHANG-jx/DocRes) | Jun. 2024 |
| <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28366"><img src="./figs/AAAI-logo.png" width="80"></a> | `DocNLC` | SCUT | DocNLC: A Document Image Enhancement Framework with Normalized and Latent Contrastive Representation for Multiple Degradations | [![GitHub Stars](https://img.shields.io/github/stars/RylonW/DocNLC?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/RylonW/DocNLC) | Feb. 2024 |
| <a href="https://dl.acm.org/doi/10.1145/3664647.3681548"><img src="./figs/ACM-MM-logo.jpg" width="80"></a> | `Document Registration` | XJTLU | Document Registration: Towards Automated Labeling of Pixel-Level Alignment Between Warped-Flat Documents | --- | Jan. 2024 |
| <a href="https://dl.acm.org/doi/fullHtml/10.1145/3610548.3618174"><img src="./figs/SIGGRAPH-logo.jpg" width="80"></a> | `UVDoc` | ETH Zurich | UVDoc: Neural Grid-based Document Unwarping | [![GitHub Stars](https://img.shields.io/github/stars/tanguymagne/UVDoc?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/tanguymagne/UVDoc) | Oct. 2023 |
| <a href="https://openaccess.thecvf.com/content/ICCV2023W/NIVT/papers/Hertlein_Template-Guided_Illumination_Correction_for_Document_Images_with_Imperfect_Geometric_Reconstruction_ICCVW_2023_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `IIITrTemplate` | FZI | Template-guided illumination correction for document images with imperfect geometric reconstruction | [![GitHub Stars](https://img.shields.io/github/stars/FelixHertlein/illtrtemplate-model?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/FelixHertlein/illtrtemplate-model) | Oct. 2023 |
| <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Foreground_and_Text-lines_Aware_Document_Image_Rectification_ICCV_2023_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `Foreground and Text-lines Aware Model` | Harbin Institute of Technology Shenzhen, China | Foreground and text-lines aware document image rectification | [![GitHub Stars](https://img.shields.io/github/stars/xiaomore/Document-Image-Dewarping?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/xiaomore/Document-Image-Dewarping) | Jun. 2023 |
### ðŸ“„ Physical Structure Analysis
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2503.17213) | `PP-DocLayout` | PaddlePaddle Team, Baidu | PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction | [![GitHub Stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleX?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/PaddlePaddle/PaddleX) | Mar. 2025 |
| <a href="https://link.springer.com/article/10.1007/s10032-025-00556-4"><img src="./figs/IJDAR-logo.png" width="80"></a> | `DCEM-ViT` | Sharda University, Greater Noida, India | Devanagari character encoded mix-merge vision transformer for robust document layout analysis | --- | Mar. 2025 |
| <a href="https://link.springer.com/chapter/10.1007/978-3-032-04627-7_28"><img src="./figs/ICDAR-logo.png" width="80"></a> | `HiLEx` | Institute of Engineering and Management, Kolkata, India | HiLEx: Image-Based Hierarchical Layout Extraction from Question Papers | [![GitHub Stars](https://img.shields.io/github/stars/HiLEX-DLA/HiLEX?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/HiLEX-DLA/HiLEX) | Mar. 2025 |
| <a href="https://openaccess.thecvf.com/content/ICCV2025W/VisionDocs/papers/Shehzadi_DocSemi_Efficient_Document_Layout_Analysis_with_Guided_Queries_ICCVW_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `DocSemi` |  Technical University of Kaiserslautern, Germany | DocSemi: Efficient Document Layout Analysis with Guided Queries | --- | Feb. 2025 |
| <a href="https://openaccess.thecvf.com/content/ICCV2025W/WiCV/papers/Shehzadi_Efficient_Additive_Attention_for_Transformer-based_Semi-supervised_Document_Layout_Analysis_ICCVW_2025_paper.pdf"><img src="./figs/ICCV-logo.png" width="80"></a> | `Efficient Additive Attention DLA` | Technical University of Kaiserslautern, Germany | Efficient Additive Attention for Transformer-based Semi-supervised Document Layout Analysis | --- | Feb. 2025 |
| <a href="https://doi.org/10.1007/s10032-025-00539-5"><img src="./figs/IJDAR-logo.png" width="80"></a> | `FS-QCSNet` | China University of Mining and Technology | Few-Shot Quaternion-valued Correlation Squeeze Network for Document Image Layout Segmentation | --- | Jan. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2410.12628) | `DocLayout-YOLO` | OpenDataLab | DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception | [![GitHub Stars](https://img.shields.io/github/stars/opendatalab/DocLayout-YOLO?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/opendatalab/DocLayout-YOLO) | Oct. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_LayoutLLM_Layout_Instruction_Tuning_with_Large_Language_Models_for_Document_CVPR_2024_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `LayoutLLM` | Alibaba Research | LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM) | Jun. 2024 |
| <a href="https://aclanthology.org/2024.acl-long.463.pdf"><img src="./figs/ACL-logo.png" width="80"></a> | `DocLLM` | JPMorgan AI Research | DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/dswang2011/DocLLM?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dswang2011/DocLLM) | May 2024 |
| <a href="https://link.springer.com/article/10.1007/s10032-024-00473-y"><img src="./figs/IJDAR-logo.png" width="80"></a> | `SemiDocSeg` | Computer Vision Center, Barcelona, Spain | SemiDocSeg: Harnessing Semi-Supervised Learning for Document Layout Analysis | --- | Mar. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_GeoLayoutLM_Geometric_Pre-Training_for_Visual_Information_Extraction_CVPR_2023_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `GeoLayoutLM` | Alibaba Research | GeoLayoutLM: Geometric Pre-training for Visual Information Extraction | [![GitHub Stars](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/GeoLayoutLM) | Jun. 2023 |
| <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548112"><img src="./figs/ACM-MM-logo.jpg" width="80"></a> | `LayoutLMv3` | Microsoft | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/unilm?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/unilm/tree/master/layoutlmv3) | Oct. 2022 |
### ðŸ“„ Reading Order Prediction
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.10258) | `XY-cut++` | Tianjin University | XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a Novel Benchmark | - | Apr. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](http://arxiv.org/abs/2409.19672) | `ROOR` | Fudan University | Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding | [![GitHub Stars](https://img.shields.io/github/stars/chongzhangFDU/ROOR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chongzhangFDU/ROOR) | Sep. 2024 |
| <a href="https://aclanthology.org/2023.emnlp-main.846/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `Reading Order Matters` | Fudan University | Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction | [![GitHub Stars](https://img.shields.io/github/stars/chongzhangFDU/Token-Path-Prediction-Datasets?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chongzhangFDU/Token-Path-Prediction-Datasets) | Dec. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2108.11591) | `LayoutReader` | University of California, San Diego+Microsoft Research Asia| LayoutReader: Pre-training of Text and Layout for Reading Order Detection | - | Aug. 2021 |
| <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548112"><img src="./figs/ACM-MM-logo.jpg" width="80"></a> | `LayoutLMv3` | Sun Yat-sen University | LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking | [![GitHub Stars](https://img.shields.io/github/stars/microsoft/unilm?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/microsoft/unilm/tree/master/layoutlmv3) | Oct. 2022 |
### ðŸ“„ Mathematical Expression Recognition
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2508.09220) | `TexTeller` | Beijing University of Posts and Telecommunications | Towards Scalable Training for Handwritten Mathematical Expression Recognition | [![GitHub Stars](https://img.shields.io/github/stars/OleehyO/TexTeller?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/OleehyO/TexTeller) | Aug. 2025 |
| <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34396"><img src="./figs/AAAI-logo.png" width="80"></a> | `SSAN` | Inner Mongolia University | SSAN: A Symbol Spatial-Aware Network for Handwritten Mathematical Expression Recognition |  [![GitHub Stars](https://img.shields.io/github/stars/qingzhenduyu/TAMER?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/qingzhenduyu/TAMER/) | Apr. 2025|
| <a href="https://ojs.aaai.org/index.php/AAAI/article/view/33190"><img src="./figs/AAAI-logo.png" width="80"></a> | `TAMER` | Peking University |TAMER: Tree-Aware Transformer for Handwritten Mathematical Expression Recognition |  [![GitHub Stars](https://img.shields.io/github/stars/Howrunz/SSAN?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Howrunz/SSAN) | Apr. 2025|
| [![Journal](https://img.shields.io/badge/PR-green)](https://www.sciencedirect.com/science/article/pii/S0031320325000068) |`VLPG`|  University of Chinese Academy of Science |Visionâ€“language pre-training for graph-based handwritten mathematical expression recognition |  [![GitHub Stars](https://img.shields.io/github/stars/guohy17/VLPG?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/guohy17/VLPG) | Jan. 2025|
| <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32422"><img src="./figs/AAAI-logo.png" width="80"></a> | `LattE` | Purdue University | LattE: Improving LaTeX Recognition with Iterative Refinement |  [![GitHub Stars](https://img.shields.io/github/stars/lt-asset/Latte?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/lt-asset/Latte) | Sep. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yuan_Syntax-Aware_Network_for_Handwritten_Mathematical_Expression_Recognition_CVPR_2022_paper.html"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `SAN (Syntax-Aware Net)` | Tomorrow Advancing Life | Syntax-Aware Network for Handwritten Mathematical Expression Recognition | [![GitHub Stars](https://img.shields.io/github/stars/Ashwinr-07/Syntax-Aware-Network?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Ashwinr-07/Syntax-Aware-Network)  | Jun. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://dl.acm.org/doi/pdf/10.5555/3524938.3525965) | `TSD (Tree-Structured Decoder)` | University of Science and Technology of China | A Tree-Structured Decoder for Image-to-Markup Generation | [![GitHub Stars](https://img.shields.io/github/stars/JianshuZhang/TreeDecoder?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/JianshuZhang/TreeDecoder)  | Jul. 2020 |
| - | `pix2tex (LaTeX-OCR)` | - | - | [![GitHub Stars](https://img.shields.io/github/stars/lukas-blecher/LaTeX-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/lukas-blecher/LaTeX-OCR) | 2021 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2105.02412) | `BTTR` | Peking University | Handwritten MER with Bidirectionally Trained Transformer | [![GitHub Stars](https://img.shields.io/github/stars/Green-Wood/BTTR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Green-Wood/BTTR) | May 2021 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://www.mdpi.com/2076-3417/11/16/7610) | - | Technical University, 67663 Kaiserslautern, Germany | Cascade Network with Deformable Composite Backbone for Formula Detection in Scanned Document Images | - | 2021 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://link.springer.com/article/10.1007/s11263-020-01291-5) | `PAL-v2` |  Institute of Automation of Chinese Academy of Sciences, China | Handwritten Mathematical Expression Recognition via Paired Adversarial Learning | - | 2020 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2003.08005) | `ScanSSD` |  Rochester Institute of Technology | ScanSSD: Scanning Single Shot Detector for Mathematical Formulas in PDF Document Images | [![GitHub Stars](https://img.shields.io/github/stars/MaliParag/ScanSSD?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/MaliParag/ScanSSD) | Mar. 2020 |
### ðŸ“„ Geometry Problem-solving
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org) | `GeoFocus` | HUST & Xiaomi | GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving | [![GitHub Stars](https://img.shields.io/github/stars/dle666/GeoFocus?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dle666/GeoFocus) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org) | `Rec-CoT` | Beihang | Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving | --- | Feb. 2026 |
| <img src="./figs/ICLR-logo.png" width="80"> | `AutoGPS` | Xiâ€™an Jiaotong | AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning | [![GitHub Stars](https://img.shields.io/github/stars/jayce-ping/AutoGPS-homepage?style=for-the-badge&logo=github&label=GitHub&color=black)](https://jayce-ping.github.io/AutoGPS-homepage/) | Jan. 2026 |
| <img src="./figs/ACM-logo.png" width="80"> | `GeoLogic` | University of Science and Technology of China | Enhancing the Geometric Problem Solving Ability of Multimodal LLMs via Symbolic-Neural Integration | [![GitHub Stars](https://img.shields.io/github/stars/ycpNotFound/GeoGen?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ycpNotFound/GeoGen) | Oct. 2025 |
| <img src="./figs/EMNLP-logo.png" width="80"> | `TR-CoT` | HUST & Baidu | Theorem-Validated Reverse Chain-of-Thought Problem Generation for Geometric Reasoning | [![GitHub Stars](https://img.shields.io/github/stars/dle666/R-CoT?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dle666/R-CoT) | Nov. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org) | `GF-Reasoner` | The University of Hong Kong | Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving | [![GitHub Stars](https://img.shields.io/github/stars/TianyunYoung/GF-Reasoner?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/TianyunYoung/GF-Reasoner) | Aug. 2025 |
| <img src="./figs/ACM-logo.png" width="80"> | `GeoUni` | Peking University | GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions | [![GitHub Stars](https://img.shields.io/github/stars/chengruogu0915/GeoUni?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chengruogu0915/GeoUni) | Apr. 2025 |
| <img src="./figs/AAAI-logo.png" width="80"> | `GNS` | Xiâ€™an Jiaotong | GNS: Solving Plane Geometry Problems by Neural Symbolic Reasoning with Multimodal LLMs | [![GitHub Stars](https://img.shields.io/github/stars/ning-mz/GNS?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ning-mz/GNS) | Apr. 2025 |
| <img src="./figs/NAACL-logo.png" width="80"> | `Geocoder` | Mila | Geocoder: Solving Geometry Problems by Generating Modular Code through Vision-Language Models | --- | Apr. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org) | `Pi-GPS` | Beijing Normal University | Pi-GPS: Enhancing Geometry Problem Solving by Unleashing the Power of Diagrammatic Information | --- | Mar. 2025 |
| <img src="./figs/ICASSP-logo.png" width="80"> | `DFE-GPS` | Shanghai Jiao Tong | Diagram Formalization Enhanced Multi-modal Geometry Problem Solver | [![GitHub Stars](https://img.shields.io/github/stars/zezeze97/DFE-GPS?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/zezeze97/DFE-GPS) | Mar. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org) | `alphageometry2` | Google DeepMind | Gold-Medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 | [![GitHub Stars](https://img.shields.io/github/stars/google-deepmind/alphageometry2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/google-deepmind/alphageometry2) | Feb. 2025 |
| <img src="./figs/ACM-logo.png" width="80"> | `GeoLogic` | University of Science and Technology of China | Enhancing the Geometric Problem Solving Ability of Multimodal LLMs via Symbolic-Neural Integration | [![GitHub Stars](https://img.shields.io/github/stars/ycpNotFound/GeoGen?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ycpNotFound/GeoGen) | Oct. 2025 |
| <img src="./figs/ICCWAMTIP-logo.png" width="80"> | `Geo-Qwen` | University of Electronic Science and Technology of China | Geo-Qwen: A Geometry Problem Solving Method Based on Generative Large Language Models and Heuristic Reasoning | --- | Dec. 2024 |
| <img src="./figs/ACM-logo.png" width="80"> | `Geo-Llava` | Huawei | Geo-LLaVA: A Large Multi-modal Model for Solving Geometry Math Problems with Meta In-Context Learning | --- | Dec. 2024 |
| <img src="./figs/ICLR-logo.png" width="80"> | `GeoX` | Shanghai Jiao Tong | GeoX: Geometric Problem Solving through Unified Formalized Vision-Language Pre-training | [![GitHub Stars](https://img.shields.io/github/stars/InternScience/GeoX?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/InternScience/GeoX) | Dec. 2024 |
| <img src="./figs/ACL-logo.png" width="80"> | `Beyond Lines and Circles` | Google | Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models | --- | Nov. 2024 |
| <img src="./figs/ACM-logo.png" width="80"> | `Reason-and-Execute` | Guangdong Smart Education Research Institute | Reason-and-Execute Prompting: Enhancing Multimodal Large Language Models for Solving Geometry Questions | [![GitHub Stars](https://img.shields.io/github/stars/chanllon/RaE.-Reason-and-Execute-Prompting?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/chanllon/RaE.-Reason-and-Execute-Prompting) | Oct. 2024 |
| <img src="./figs/NeurIPS-logo.png" width="80"> | `Formal Representation and Solution` | Shanghai University | Formal Representation and Solution of Plane Geometric Problems | --- | Oct. 2024 |
| <img src="./figs/EMNLP-logo.png" width="80"> | `Two-step CoT` | University of Science and Technology of China | Multi-step Chain-of-Thought in Geometry Problem Solving | --- | Sep. 2024 |
| <img src="./figs/ACM-MM-logo.jpg" width="80"> | `MM-RAG` | IIIT | Advancing Multimodal LLMs: A Focus on Geometry Problem Solving Reasoning and Sequential Scoring | --- | Sep. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org) | `GeoGPT4V` | University of Science and Technology of China | GeoGPT4V: Towards Geometric Multimodal Large Language Models with Geometric Image Generation | [![GitHub Stars](https://img.shields.io/github/stars/alibaba/GeoGPT4V?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alibaba/GeoGPT4V) | Jun. 2024 |
| <img src="./figs/CVPR-logo.jpg" width="80"> | `E-GPS` | Xiâ€™an Jiaotong | E-GPS: Explainable Geometry Problem Solving via Top-Down Solver and Bottom-Up Generator | [![GitHub Stars](https://img.shields.io/github/stars/majianz/dl4gps?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/majianz/dl4gps) | May 2024 |
| <img src="./figs/Symmetry-logo.png" width="80"> | `FGPS` | Shanghai University | FGeo-SSS: A Search-Based Symbolic Solver for Human-Like Automated Geometric Reasoning | --- | Mar. 2024 |
| <img src="./figs/Nature-logo.png" width="80"> | `AlphaGeometry` | Google DeepMind | Solving Olympiad Geometry without Human Demonstrations | [![GitHub Stars](https://img.shields.io/github/stars/google-deepmind/alphageometry2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/google-deepmind/alphageometry2) | Jan. 2024 |



### ðŸ“„ Chart Understanding
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Journal](https://img.shields.io/badge/IPM-blue)](https://www.sciencedirect.com/science/article/pii/S0306457325005497) | `ChartReLA`|University of Science, Ho Chi Minh city, Vietnam|ChartReLA: A compact vision-language model for comprehensivechart reasoning via relationship modeling|  [![GitHub Stars](https://img.shields.io/github/stars/nxquang-al/ChartReLA?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/nxquang-al/ChartReLA) |Jan. 2026|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2601.05688) | `SketchVL` | Xiâ€™an Jiaotong University | SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More | - | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2601.18238) | `LLama-VL-TUG` | IIT Kanpur | TechING: Towards Real World Technical Image Understanding via VLMs | - | Jan. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2512.07186) | `START` | University of Wisconsin-Madison | START: Spatial and Textual Learning for Chart Understanding | [![GitHub Stars](https://img.shields.io/github/stars/dragonlzm/START?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/dragonlzm/START) | Dec. 2025 |
| <a href="https://link.springer.com/chapter/10.1007/978-3-032-04627-7_30"><img src="./figs/ICDAR-logo.png" width="80"></a> | `RefChartQA` |  Karlsruhe Institute of Technology| RefChartQA: Grounding Visual Answer on Chart Images through Instruction Tuning | [![GitHub Stars](https://img.shields.io/github/stars/moured/RefChartQA?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/moured/RefChartQA) | Jan. 2025 |
| <a href="https://aclanthology.org/2025.findings-emnlp.172/"><img src="./figs/EMNLP-logo.png" width="80"></a> | `ReachQA` | Fudan University | Distill Visual Chart Reasoning Ability from LLMs to MLLMs | [![GitHub Stars](https://img.shields.io/github/stars/hewei2001/ReachQA?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/hewei2001/ReachQA) | Oct. 2024  |
|  <a href="https://proceedings.iclr.cc/paper_files/paper/2025/hash/c33cd281f8cd784626a57de340e43fe4-Abstract-Conference.html"><img src="./figs/ICLR-logo.png" width="80"></a>| `ChartMoE` | IDEA Research | ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding | [![GitHub Stars](https://img.shields.io/github/stars/IDEA-FinAI/ChartMoE?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/IDEA-FinAI/ChartMoE) | Sep. 2024 |
| <a href="https://link.springer.com/chapter/10.1007/978-3-031-78107-0_24"><img src="./figs/ICPR-logo.png" width="80"></a> | `C2F-CHART` | Alexandria University | C2F-CHART: A Curriculum Learning Approach to Chart Classification | - | Aug. 2024 |
| <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wan_OmniParser_A_Unified_Framework_for_Text_Spotting_Key_Information_Extraction_CVPR_2024_paper.pdf"><img src="./figs/CVPR-logo.jpg" width="80"></a> | `OmniParser` | Alibaba | OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition | [![GitHub Stars](https://img.shields.io/github/stars/AlibabaResearch/AdvancedLiterateMachinery?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/AlibabaResearch/AdvancedLiterateMachinery) | Jun. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2305.14761) | `UniChart` | York University | UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning | [![GitHub Stars](https://img.shields.io/github/stars/vis-nlp/UniChart?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/vis-nlp/UniChart) | May. 2023 |
| <a href="https://aclanthology.org/2023.findings-acl.660/"><img src="./figs/ACL-logo.png" width="80"></a> | `DePlot` | Google | DePlot: One-shot visual language reasoning by plot-to-table translation | [![GitHub Stars](https://img.shields.io/github/stars/google-research/google-research?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/google-research/google-research/tree/master/deplot) | Jul. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2212.09662) | `MatCha` | Google | MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering | - | Dec. 2022 |
| <a href="https://aclanthology.org/2022.findings-acl.177/"><img src="./figs/ACL-logo.png" width="80"></a> | `ChartQA` | York University | ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning | [![GitHub Stars](https://img.shields.io/github/stars/vis-nlp/ChartQA?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/vis-nlp/ChartQA) | Jul. 2022 |
### ðŸ“„ Scene Text Spotting
| Venue | Name | Primary affiliation | Title  | GitHub | Date |
|:-:|:-:|:-:|:-:|:-:|:-:|
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2602.04030) | `TiCLS` | University of Minnesota | TiCLS: Tightly Coupled Language Text Spotter | [![GitHub Stars](https://img.shields.io/github/stars/knowledge-computing/TiCLS?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/knowledge-computing/TiCLS) | Feb. 2026 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2504.09966) | `SemiETS` | Huazhong University of Science and Technology | SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/DrLuo/SemiETS?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/DrLuo/SemiETS) | Apr. 2025 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2412.15523) | `InstructOCR` | Meituan | InstructOCR: Instruction Boosting Scene Text Spotting | - | Dec. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.14998) | `FastTextSpotter` | Indian Statistical Institute | FastTextSpotter: A High-Efficiency Transformer for Multilingual Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/alloydas/Fast-Textspotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/alloydas/Fast-Textspotter) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2408.00355) | `DNTextSpotter` | Soochow University | DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training | [![GitHub Stars](https://img.shields.io/github/stars/yyyyyxie/DNTextSpotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/yyyyyxie/DNTextSpotter) | Aug. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2404.00852) | `Ensemble Learning` | University of Science, VNU-HCM | Ensemble Learning for Vietnamese Scene Text Spotting in Urban Environments | - | Apr. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2404.04624) | `Bridging Text Spotting` | South China University of Technology | EBridging the Gap Between End-to-End and Two-Step Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/mxin262/Bridging-Text-Spotting?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mxin262/Bridging-Text-Spotting) | Apr. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2403.10047) | `TextBlockV2` | Chinese Academy of Sciences | TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model | - | Mar. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2402.17134) | `Linguistic Priors` | University of Rochester | Efficiently Leveraging Linguistic Priors for Scene Text Spotting | - | Feb. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2401.07641) | `SwinTextSpotter v2` | South China University of Technology | SwinTextSpotter v2: Towards Better Synergy for Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/mxin262/SwinTextSpotterv2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mxin262/SwinTextSpotterv2) | Jan. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2401.03637) | `IATS` | University of Science and Technology Beijing | Inverse-like Antagonistic Scene Text Spotting via Reading-Order Estimation and Dynamic Sampling | - | Jan. 2024 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2312.15690) | `WordLenSpotter` | Wuhan Institute of Technology | Word length-aware text spotting: Enhancing detection and recognition in dense text image | - | Dec. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2309.02356) | `STEP` | Computer Vision Center, UAB | STEP -- Towards Structured Scene-Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/CVC-DAG/STEP?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/CVC-DAG/STEP) | Sep. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2308.10147) | `ESTextSpotter` | South China University of Technology | ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy in Transformer | [![GitHub Stars](https://img.shields.io/github/stars/mxin262/ESTextSpotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mxin262/ESTextSpotter) | Aug. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2304.03435) | `UNITS` | Naver Cloud | Towards Unified Scene Text Spotting based on Sequence Generation | [![GitHub Stars](https://img.shields.io/github/stars/clovaai/units?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/clovaai/units) | Apr. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2302.10641) | `A3S` | Fast Accounting Co., Ltd. | A3S: Adversarial learning of semantic representations for Scene-Text Spotting | - | Feb. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2301.01635) | `SPTS v2` | Huazhong University of Science and Technology | SPTS v2: Single-Point Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/Yuliang-Liu/SPTSv2?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Yuliang-Liu/SPTSv2) | Jan. 2023 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2211.10578) | `ABINet++` | University of Science and Technology of China | ABINet++: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/FangShancheng/ABINet-PP?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/FangShancheng/ABINet-PP) | Nov. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/pdf/2211.10772) | `DeepSolo` | Wuhan University | DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/ViTAE-Transformer/DeepSolo?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/ViTAE-Transformer/DeepSolo) | Nov. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://dl.acm.org/doi/pdf/10.1145/3503161.3547882) | `TPSNet` | Institute of Information Engineering | TPSNet: Reverse Thinking of Thin Plate Splines for Arbitrary Shape Scene Text Representation | [![GitHub Stars](https://img.shields.io/github/stars/Wei-ucas/TPSNet?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/Wei-ucas/TPSNet) | Nov. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2208.03364) | `GLASS` | Technion | GLASS: Global to Local Attention for Scene-Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/amazon-science/glass-text-spotting?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/amazon-science/glass-text-spotting) | Aug. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2203.10209) | `SwinTextSpotter` | South China University of Technology | SwinTextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition | [![GitHub Stars](https://img.shields.io/github/stars/mxin262/SwinTextSpotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/mxin262/SwinTextSpotter) | Mar. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2203.05122) | `DEER` | Naver Clova | DEER: Detection-agnostic End-to-End Recognizer for Scene Text Spotting | - | Mar. 2022 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2112.07917) | `SPTS` | South China University of Technology | SPTS: Single-Point Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/shannanyinxiang/SPTS?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/shannanyinxiang/SPTS) | Dec. 2021 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2105.00405) | `PAN++` | Nanjing University | PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text | [![GitHub Stars](https://img.shields.io/github/stars/whai362/pan_pp.pytorch?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whai362/pan_pp.pytorch) | May. 2021 |    
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2105.03620) | `ABCNet v2` | South China University of Technology | ABCNet v2: Adaptive Bezier-Curve Network for Real-time End-to-end Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/aim-uofa/AdelaiDet?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/aim-uofa/AdelaiDet) | May. 2021 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2012.04350) | `MANGO` | Hikvision Research Institute | MANGO: A Mask Attention Guided One-Stage Scene Text Spotter | [![GitHub Stars](https://img.shields.io/github/stars/hikopensource/DAVAR-Lab-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/hikopensource/DAVAR-Lab-OCR) | Dec. 2020 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2008.00714) | `AE TextSpotter` | Nanjing University | AE TextSpotter: Learning Visual and Linguistic Representation for Ambiguous Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/whai362/AE_TextSpotter?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/whai362/AE_TextSpotter) | Aug. 2020 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2007.09482) | `Mask TextSpotter v3` | Huazhong University of Science and Technology | Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting | [![GitHub Stars](https://img.shields.io/github/stars/MhLiao/MaskTextSpotterV3?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/MhLiao/MaskTextSpotterV3) | Jul. 2020 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://aaai.org/ojs/index.php/AAAI/article/view/6864/6718) | `Text perceptron` | Hikvision Research Institute | Text perceptron: Towards end-to-end arbitrary-shaped text spotting | [![GitHub Stars](https://img.shields.io/github/stars/hikopensource/DAVAR-Lab-OCR?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/hikopensource/DAVAR-Lab-OCR) | Apr. 2020 |
| [![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2002.10200) | `ABCNet` | South China University of Technology | ABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network | [![GitHub Stars](https://img.shields.io/github/stars/aim-uofa/AdelaiDet?style=for-the-badge&logo=github&label=GitHub&color=black)](https://github.com/aim-uofa/AdelaiDet) | Feb. 2020 |


## All Thanks To Our Contributors

<a href="https://github.com/Yuliang-Liu/OCR-LLM-ERA/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=Yuliang-Liu/OCR-LLM-ERA" />
</a>

We welcome contributions from the community and encourage pull requests to help keep this project up to date. Suggestions, feedback, and corrections are always welcome â€” please feel free to share them anytime!
